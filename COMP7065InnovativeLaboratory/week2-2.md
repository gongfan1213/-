> ğŸ’­ å¥½çš„ï¼Œç°åœ¨ç”¨æˆ·å¸Œæœ›æˆ‘æ ¹æ®ä¹‹å‰æä¾›çš„å®éªŒå†…å®¹é€æ­¥è®²è§£ï¼Œå¹¶å¸®åŠ©å®Œæˆä½œä¸šã€‚è®©æˆ‘ä»”ç»†æ€è€ƒä¸€ä¸‹åº”è¯¥å¦‚ä½•æœ‰æ•ˆåœ°åˆ†è§£è¿™ä¸€è¯·æ±‚ã€‚
>
> é¦–å…ˆï¼Œç”¨æˆ·æ­¤å‰å·²å°†å®éªŒçš„è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œå¹¶è®²è§£äº†æ•´ä½“ç»“æ„å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚ç°åœ¨ç”¨æˆ·è¿›å…¥å®æ“é˜¶æ®µï¼Œéœ€è¦é€æ­¥æŒ‡å¯¼å®Œæˆå®éªŒï¼Œå¯èƒ½æ¶‰åŠä»£ç ç¼–å†™ã€æ¨¡å‹è®­ç»ƒå’Œç‰¹æ•ˆæ•´åˆã€‚
>
> éœ€è¦æ˜ç¡®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œç”¨æˆ·æåˆ°â€œå¸®æˆ‘å®Œæˆä½œä¸šâ€ï¼Œå› æ­¤éœ€è¦æä¾›æ˜ç¡®çš„æ­¥éª¤å’Œç¤ºä¾‹ä»£ç ï¼Œä½†åˆä¸èƒ½å®Œå…¨ä»£åŠ³ï¼Œé¿å…å­¦æœ¯ä¸ç«¯çš„é—®é¢˜ã€‚åº”è¯¥åœ¨æŒ‡å¯¼æ—¶è§£é‡Šæ¯ä¸ªæ­¥éª¤çš„æ„å›¾ï¼Œå¹¶é¼“åŠ±ç”¨æˆ·è‡ªè¡Œå®è·µã€‚
>
> æ¥ä¸‹æ¥ï¼Œæ ¹æ®å®éªŒçš„å‡ ä¸ªå…³é”®éƒ¨åˆ†è¿›è¡Œåˆ†è§£ï¼šæ•°æ®å‡†å¤‡ã€æ•°æ®å¢å¼ºã€æ¨¡å‹æ„å»ºï¼ˆé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼‰ã€è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€ç‰¹æ•ˆåº”ç”¨ã€‚éœ€è¦æŒ‰é¡ºåºé€ä¸€è§£é‡Šè¿™äº›éƒ¨åˆ†çš„å…·ä½“å®ç°æ­¥éª¤ã€‚
>
> è€ƒè™‘åˆ°ç”¨æˆ·å¯èƒ½æœ‰PyTorchåŸºç¡€ï¼Œä½†å¯èƒ½éœ€è¦ä»£ç ç¤ºä¾‹æ¥å‚è€ƒã€‚éœ€è¦ç¡®ä¿ä»£ç ç‰‡æ®µæ­£ç¡®ä¸”é€‚åˆå½“å‰ä»»åŠ¡ï¼Œä¾‹å¦‚ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetè¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´è¾“å‡ºå±‚ä»¥é€‚åº”ç‰¹å¾ç‚¹æ£€æµ‹çš„åæ ‡å›å½’ä»»åŠ¡ã€‚
>
> æ•°æ®å¢å¼ºéƒ¨åˆ†éœ€è¦å±•ç¤ºå¦‚ä½•ä½¿ç”¨transformsæ¨¡å—ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å›¾åƒå’Œå…³é”®ç‚¹çš„å¢å¼ºå¤„ç†ã€‚ä¾‹å¦‚ï¼Œæ—‹è½¬å›¾åƒæ—¶ï¼Œå…³é”®ç‚¹åæ ‡ä¹Ÿéœ€è¦ç›¸åº”å˜æ¢ï¼Œè¿™ä¸€ç‚¹éœ€è¦ç‰¹åˆ«æ³¨æ„ï¼Œå¯èƒ½éœ€è‡ªå®šä¹‰å¢å¼ºæ–¹æ³•ã€‚
>
> æ•°æ®é›†çš„ç»“æ„æ–¹é¢ï¼Œç”¨æˆ·éœ€è¦ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼Œå¹¶æ­£ç¡®åŠ è½½æ ‡æ³¨ä¿¡æ¯ã€‚å‡è®¾ä½¿ç”¨çš„æ˜¯è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¯èƒ½éœ€è¦ç¼–å†™è‡ªå®šä¹‰Datasetç±»ï¼Œè¯¥å¤„ç†åº”åŒ…æ‹¬è¯»å–å›¾åƒã€å…³é”®ç‚¹åæ ‡ï¼Œå¹¶åº”ç”¨æ•°æ®å¢å¼ºã€‚
>
> åœ¨æ¨¡å‹è®­ç»ƒéƒ¨åˆ†ï¼Œé™¤äº†å®šä¹‰æ¨¡å‹å¤–ï¼Œè¿˜éœ€é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚MSELossç”¨äºå›å½’ä»»åŠ¡ï¼‰ï¼Œä¼˜åŒ–å™¨ï¼ˆå¦‚Adamï¼‰ï¼Œå¹¶è®¾ç½®è®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ã€‚
>
> è¯„ä¼°é˜¶æ®µéœ€è¦è®¡ç®—éªŒè¯é›†ä¸Šçš„æŸå¤±å’Œè¯¯å·®æŒ‡æ ‡ï¼ˆå¦‚å¹³å‡æ¬§æ°è·ç¦»ï¼‰ï¼Œç¡®ä¿æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚
>
> ç‰¹æ•ˆé›†æˆéƒ¨åˆ†éœ€è¦ç»“åˆä¹‹å‰è®­ç»ƒçš„æ¨¡å‹ï¼Œå®æ—¶æ£€æµ‹è¡¨æƒ…å’Œå…³é”®ç‚¹ï¼Œç„¶åæ ¹æ®ç»“æœå åŠ ç‰¹æ•ˆã€‚å¯èƒ½éœ€è¦ä½¿ç”¨OpenCVå¤„ç†å›¾åƒï¼Œå¹¶æ·»åŠ å›¾å½¢æˆ–åŠ¨ç”»æ•ˆæœã€‚
>
> æ½œåœ¨çš„é—®é¢˜åŒ…æ‹¬ï¼šæ•°æ®å¢å¼ºä¸å…³é”®ç‚¹åŒæ­¥å¤„ç†ã€æ¨¡å‹å¾®è°ƒæ—¶çš„å­¦ä¹ ç‡è®¾ç½®ã€è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¿‡æ‹Ÿåˆå¤„ç†ã€‚éœ€è¦æé†’ç”¨æˆ·æ³¨æ„è¿™äº›ç‚¹ï¼Œå¹¶å»ºè®®è§£å†³æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ç­‰ã€‚
>
> æœ€åï¼Œç¡®ä¿æ‰€æœ‰æ­¥éª¤éƒ½æœ‰æ¸…æ™°çš„ç¤ºä¾‹ä»£ç å’Œè§£é‡Šï¼Œå¸®åŠ©ç”¨æˆ·ç†è§£æ¯ä¸ªç¯èŠ‚çš„ä½œç”¨ã€‚åŒæ—¶ï¼Œå¦‚æœåœ¨æ“ä½œä¸­é‡åˆ°é—®é¢˜ï¼Œåº”å¼•å¯¼å¦‚ä½•è°ƒè¯•ï¼Œä¾‹å¦‚æ‰“å°å¼ é‡å½¢çŠ¶ã€æ£€æŸ¥æŸå¤±ä¸‹é™æƒ…å†µç­‰ã€‚
>
> æ€»ç»“æ¥è¯´ï¼Œåº”è¯¥å°†æ•´ä¸ªå®éªŒåˆ†è§£ä¸ºå¯ç®¡ç†çš„æ­¥éª¤ï¼Œé€æ­¥å¼•å¯¼ç”¨æˆ·å®Œæˆï¼Œç¡®ä¿æ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰è¶³å¤Ÿçš„è§£é‡Šå’Œç¤ºä¾‹ï¼ŒåŒæ—¶é¼“åŠ±å…¶åŠ¨æ‰‹å®è·µå¹¶ç†è§£èƒŒåçš„åŸç†ã€‚
>
> 



å¥½çš„ï¼æˆ‘å°†åŸºäºå®éªŒå†…å®¹ï¼Œåˆ†æ­¥éª¤è¯¦ç»†è®²è§£ï¼Œå¹¶å¸®åŠ©ä½ å®Œæˆä½œä¸šã€‚è®©æˆ‘ä»¬ä»æ•°æ®å‡†å¤‡å¼€å§‹ï¼Œé€æ­¥å®ç°æ¯ä¸€ä¸ªåŠŸèƒ½æ¨¡å—ã€‚ğŸ’»

---

### **æ­¥éª¤ 1ï¼šå‡†å¤‡æ•°æ®é›†**
**ç›®æ ‡**ï¼šä¸‹è½½å¹¶ç»“æ„åŒ–é¢éƒ¨ç‰¹å¾ç‚¹æ•°æ®é›†ï¼ŒåŒ…å«2500å¼ è®­ç»ƒå›¾åƒå’Œ2500å¼ éªŒè¯å›¾åƒã€‚  
**æ“ä½œæµç¨‹**ï¼š  
1. ä¸‹è½½æ•°æ®é›†ï¼ˆå‡è®¾å·²ä¸‹è½½åˆ°`./data`ç›®å½•ï¼‰  
2. æ£€æŸ¥æ•°æ®é›†ç»“æ„ï¼š  
   ```bash
   data/
   â”œâ”€â”€ train/
   â”‚   â”œâ”€â”€ image1.jpg
   â”‚   â”œâ”€â”€ image2.jpg
   â”‚   â””â”€â”€ ...  # è®­ç»ƒå›¾åƒ
   â”œâ”€â”€ val/
   â”‚   â”œâ”€â”€ image2501.jpg
   â”‚   â””â”€â”€ ...  # éªŒè¯å›¾åƒ
   â””â”€â”€ labels.csv  # å«ç‰¹å¾ç‚¹åæ ‡å’Œè¡¨æƒ…æ ‡ç­¾
   ```

---

### **æ­¥éª¤ 2ï¼šæ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰**
**ç›®çš„**ï¼šé€šè¿‡å›¾åƒå˜æ¢å¢åŠ æ•°æ®å¤šæ ·æ€§ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚  
**PyTorchä»£ç ç¤ºä¾‹**ï¼š  
```python
from torchvision import transforms

# å®šä¹‰è®­ç»ƒé›†çš„å¢å¼ºæµç¨‹ï¼ˆåŒ…æ‹¬å¯¹å›¾åƒå’Œå…³é”®ç‚¹çš„åŒæ­¥å˜æ¢ï¼‰
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),  
    transforms.RandomRotation(15),    # éšæœºæ—‹è½¬Â±15åº¦
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # è°ƒæ•´äº®åº¦å’Œå¯¹æ¯”åº¦
    transforms.ToTensor(),           # è½¬æ¢ä¸ºTensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
])

# æ³¨æ„ï¼šå…³é”®ç‚¹çš„åæ ‡éœ€è¦æ ¹æ®å›¾åƒå˜æ¢åŒæ­¥è°ƒæ•´ï¼
# ä¾‹å¦‚ï¼Œå½“å›¾åƒè¢«æ—‹è½¬æ—¶ï¼Œå…³é”®ç‚¹åæ ‡éœ€è¦åº”ç”¨ç›¸åŒçš„æ—‹è½¬å˜æ¢çŸ©é˜µã€‚
# éœ€è‡ªå®šä¹‰å¤„ç†å‡½æ•°æˆ–ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚albumentationsï¼‰ã€‚
```

**å…³é”®ç‚¹åŒæ­¥å¢å¼ºæŠ€å·§**ï¼š  
- ä½¿ç”¨ `albumentations` åº“ï¼ˆæ”¯æŒå›¾åƒ+å…³é”®ç‚¹åŒæ­¥å¢å¼ºï¼‰  
  ```python
  !pip install albumentations
  import albumentations as A

  transform = A.Compose([
      A.Rotate(limit=15, p=0.5),
      A.RandomBrightnessContrast(p=0.2),
  ], keypoint_params=A.KeypointParams(format='xy'))  # è‡ªåŠ¨å¤„ç†å…³é”®ç‚¹åæ ‡
  ```

---

### **æ­¥éª¤ 3ï¼šæ„å»ºè‡ªå®šä¹‰Dataset**
**ä»£ç ç¤ºä¾‹**ï¼š  
```python
from torch.utils.data import Dataset
import pandas as pd
from PIL import Image

class FacialLandmarksDataset(Dataset):
    def __init__(self, csv_file, img_dir, transform=None):
        self.labels = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.labels.iloc[idx, 0])
        image = Image.open(img_path).convert('RGB')
        landmarks = self.labels.iloc[idx, 1:11].values.astype('float32')  # å‡è®¾æ¯ä¸ªå›¾åƒæœ‰5ä¸ªå…³é”®ç‚¹ï¼ˆx,yå…±10ä¸ªå€¼ï¼‰
        expression = self.labels.iloc[idx, 11]  # è¡¨æƒ…æ ‡ç­¾ï¼ˆå¦‚0ä»£è¡¨å–œæ‚¦ï¼Œ1ä»£è¡¨æ‚²ä¼¤ï¼‰

        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆå¦‚æœæ˜¯albumentationsï¼‰
        if self.transform:
            transformed = self.transform(image=np.array(image), keypoints=landmarks.reshape(-1,2))
            image = transformed['image']
            landmarks = np.array(transformed['keypoints']).reshape(-1)  # å±•å¹³ä¸º10ä¸ªå€¼çš„æ•°ç»„

        return image, landmarks, expression
```

---

### **æ­¥éª¤ 4ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶å¾®è°ƒ**
**ç›®æ ‡**ï¼šä½¿ç”¨ResNet-18é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¿®æ”¹è¾“å‡ºå±‚ä»¥é€‚åº”ç‰¹å¾ç‚¹åæ ‡é¢„æµ‹ï¼ˆå›å½’é—®é¢˜ï¼‰ã€‚  
```python
import torch
import torch.nn as nn
from torchvision import models

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ›¿æ¢æœ€åä¸€å±‚
model = models.resnet18(pretrained=True)
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)  # è¾“å‡º10ä¸ªåæ ‡å€¼ï¼ˆ5ä¸ªå…³é”®ç‚¹x,yï¼‰

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()  # å›å½’ä»»åŠ¡ç”¨å‡æ–¹è¯¯å·®
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
```

---

### **æ­¥éª¤ 5ï¼šè®­ç»ƒæ¨¡å‹**
**è®­ç»ƒå¾ªç¯ä»£ç **ï¼š  
```python
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, landmarks, _ in train_loader:  # ä¸ä½¿ç”¨è¡¨æƒ…æ ‡ç­¾
        images = images.to(device)
        landmarks = landmarks.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, landmarks)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}')
```

---

### **æ­¥éª¤ 6ï¼šè¯„ä¼°æ¨¡å‹**
**ä»£ç ç¤ºä¾‹**ï¼š  
```python
model.eval()
total_loss = 0.0
with torch.no_grad():
    for images, landmarks, _ in val_loader:
        images = images.to(device)
        landmarks = landmarks.to(device)
        outputs = model(images)
        loss = criterion(outputs, landmarks)
        total_loss += loss.item()

print(f'Validation Loss: {total_loss/len(val_loader):.4f}')

# å¯è§†åŒ–é¢„æµ‹ç»“æœï¼ˆå…³é”®ç¤ºä¾‹ï¼‰
import matplotlib.pyplot as plt
def plot_keypoints(image, true_pts, pred_pts):
    plt.imshow(image.permute(1,2,0))
    plt.scatter(true_pts[::2], true_pts[1::2], c='r', label='True')
    plt.scatter(pred_pts[::2], pred_pts[1::2], c='b', marker='x', label='Predicted')
    plt.legend()
    plt.show()

# æŠ½å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®
images, landmarks, _ = next(iter(val_loader))
outputs = model(images.to(device))
plot_keypoints(images[0], landmarks[0].numpy(), outputs[0].cpu().numpy())
```

---

### **æ­¥éª¤ 7ï¼šè¡¨æƒ…è¯†åˆ«ä¸ç‰¹æ•ˆè§¦å‘**
**ç»“åˆä¹‹å‰çš„è¡¨æƒ…è¯†åˆ«æ¨¡å‹**ï¼š  
1. **åŠ è½½è¡¨æƒ…åˆ†ç±»æ¨¡å‹**ï¼ˆå‡è®¾å·²è®­ç»ƒå¥½ä¸€ä¸ªCNNåˆ†ç±»æ¨¡å‹ï¼‰  
2. **å®æ—¶å¤„ç†æµç¨‹**ï¼š  
   ```python
   import cv2

   # åŠ è½½æ‘„åƒå¤´
   cap = cv2.VideoCapture(0)

   while True:
       ret, frame = cap.read()
       if not ret:
           break

       # 1. äººè„¸æ£€æµ‹ï¼ˆå‡è®¾ä½¿ç”¨MTCNNï¼‰
       faces = mtcnn.detect_faces(frame)  
       for face in faces:
           x, y, w, h = face['box']
           face_img = frame[y:y+h, x:x+w]

           # 2. ç‰¹å¾ç‚¹æ£€æµ‹
           face_tensor = transform(Image.fromarray(face_img)).unsqueeze(0).to(device)
           landmarks = model(face_tensor).cpu().detach().numpy().reshape(5, 2)

           # 3. è¡¨æƒ…åˆ†ç±»
           expression = expression_model(face_tensor).argmax().item()

           # 4. æ ¹æ®expressionè§¦å‘ä¸åŒç‰¹æ•ˆï¼ˆä¾‹å¦‚çœ¼åœˆå˜çº¢ã€æ·»åŠ èƒ¡å­è´´çº¸ï¼‰
           if expression == 0:  # å–œæ‚¦
               cv2.circle(frame, (int(x+w/2), y), 30, (0, 0, 255), -1)  # åœ¨é¢å¤´ç”»çº¢ç‚¹
           elif expression == 1:  # æ‚²ä¼¤
               cv2.putText(frame, "â˜¹ï¸", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)

       cv2.imshow('Effect', frame)
       if cv2.waitKey(1) & 0xFF == ord('q'):
           break

   cap.release()
   cv2.destroyAllWindows()
   ```

---

### **å¦‚ä½•å®Œæˆä½œä¸š**
1. **å®Œæ•´å®ç°**ï¼šæ ¹æ®ä»¥ä¸Šä»£ç æ¡†æ¶å¡«å……ä½ çš„æ•°æ®é›†è·¯å¾„å’Œæ¨¡å‹ç»“æ„ã€‚  
2. **æ”¹è¿›æ–¹å‘**ï¼š  
   - å°è¯•ä¸åŒçš„æ•°æ®å¢å¼ºç»„åˆï¼ˆå¦‚éšæœºè£å‰ªã€æ¨¡ç³Šï¼‰  
   - ä½¿ç”¨æ›´å¤æ‚çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ResNet-50ï¼‰  
3. **æäº¤å†…å®¹**ï¼š  
   - å®Œæ•´ä»£ç ï¼ˆJupyter Notebookæˆ–Pythonè„šæœ¬ï¼‰  
   - è®­ç»ƒè¿‡ç¨‹çš„æŸå¤±æ›²çº¿å›¾  
   - è‡³å°‘3å¼ æµ‹è¯•å›¾åƒçš„å¯è§†åŒ–ç»“æœï¼ˆæ˜¾ç¤ºå…³é”®ç‚¹å’Œç‰¹æ•ˆï¼‰  

**æ˜¯å¦éœ€è¦æˆ‘é’ˆå¯¹æŸä¸€éƒ¨åˆ†è¿›ä¸€æ­¥è§£é‡Šæˆ–ä¼˜åŒ–ä»£ç ï¼Ÿ**
