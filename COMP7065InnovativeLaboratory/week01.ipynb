{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "In this lab, you will learn how to train a basic neural network to recognize emotions from facial expressions. Through a comprehensive case study, you will explore the entire training process, covering data preparation and loading, constructing the network, training it with the data, and finally testing and evaluating the performance of the trained model."
      ],
      "metadata": {
        "id": "SmE0BngfuV6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Emotion recognition using facial expressions is a fascinating application of neural networks in artificial intelligence. By analyzing facial images, we can identify emotions like happiness, sadness, and anger. This course will introduce the fundamentals of neural networks, data preprocessing, and model training with frameworks of pytorch. Participants will learn to build a model for emotion recognition, exploring its applications in areas such as human-computer interaction and mental health."
      ],
      "metadata": {
        "id": "zTYEkPqB4_wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives\n",
        "\n",
        "*   Learn the usage of jupyter nootbook and pytorch framework.\n",
        "*   Learn how to create a dataloader to load and prepocess the training data.\n",
        "*   Learn how to implement a neural network model and optimize it.\n",
        "*   Learn how to train, evaluate and test a model."
      ],
      "metadata": {
        "id": "md8dPHRC5u3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Study**"
      ],
      "metadata": {
        "id": "bQAYHoG9gRPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required library"
      ],
      "metadata": {
        "id": "JcTGHqnWGGqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izIG-8hmFEGW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from time import time\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the GPU availability. If the output result of `device` is cpu not cuda, please change the runtime type to T4 GPU in the above menu.(Remember to switch to GPU in subsequent labs)"
      ],
      "metadata": {
        "id": "_u6etgA-GEut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "edPzPF9RLJNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "x9ZiN2SCtM5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data.\n",
        "\n",
        "The Real-world Affective Faces Database (RAF-DB) is a dataset for facial expression. This version contains 15000 facial images. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects)\n",
        "\n"
      ],
      "metadata": {
        "id": "QFw1sotXJb1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o ./raf-db-dataset.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/shuvoalok/raf-db-dataset\n",
        "\n",
        "!unzip raf-db-dataset.zip"
      ],
      "metadata": {
        "id": "DeLWh7C4nkgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the dataloader.\n",
        "\n",
        "Below is a sample data loader for the RAF-DB dataset. **Please understand how it works and develop a custom data loader tailored to your exercise dataset independently.** Please refer to this link for more detailed tutorial: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#datasets-dataloaders\n",
        "\n",
        "Key Features of the `RAFDBDataset` Class\n",
        "* Initialization:\n",
        "  When you create an instance of the `RAFDBDataset` class, you need to provide the root directory where the dataset is stored. You can also specify whether you want to load the training data or the testing data by setting the `train` parameter to `True` or `False`.\n",
        "* Loading Images and Labels: The class is designed to automatically scan through the specified directory and gather all the images along with their corresponding labels. The images are organized into subdirectories for each emotion label (from 1 to 7). This means that when you use this class, it will find all the images in the training or testing folders and keep track of them for you.\n",
        "* Dataset Length: The class includes a method to tell you how many images are available in the dataset. This is useful for setting up training loops and understanding the size of your data.\n",
        "* Retrieving Data: When you want to get a specific image and its label, you can use an index. The class provides a way to access an image by its index, loading the image from disk, converting it to the right format (RGB), and applying any transformations you specified. It then returns both the image and its associated label."
      ],
      "metadata": {
        "id": "Lt9py-N_Hq6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAFDBDataset(Dataset):\n",
        "    def __init__(self, root_dir, train=True, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        base_dir = os.path.join(self.root_dir, 'train' if self.train else 'test')\n",
        "\n",
        "        for label in range(1, 8):\n",
        "            label_dir = os.path.join(base_dir, str(label))\n",
        "            for img_name in os.listdir(label_dir):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.images.append(os.path.join(label_dir, img_name))\n",
        "                    self.labels.append(label-1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "IWIJJmzJNm3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create transform function to preprocess the image.\n",
        "* `transforms.Resize(IMAGE_SIZE)`: Resizes the input image to a specified size, which is (224, 224) in this case.\n",
        "\n",
        "* `transforms.RandomHorizontalFlip()`: Randomly flips the image horizontally to increase the diversity of the data.\n",
        "\n",
        "* `transforms.ToTensor()`: Converts a PIL image or NumPy ndarray to a FloatTensor and scales the pixel values to the range [0, 1].\n",
        "\n",
        "* `normalize`: Normalizes the image data by subtracting the mean and dividing by the standard deviation. The mean and std values provided are for the three color channels (RGB).\n",
        "\n",
        "* `transforms.RandomErasing(scale=(0.02, 0.25))`: Randomly erases a portion of the image. The scale parameter defines the range of the area ratio of the erased region to the image area."
      ],
      "metadata": {
        "id": "OIPUukEJJUeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "normalize = transforms.Normalize(mean=[0.5752, 0.4495, 0.4012],\n",
        "                                    std=[0.2086, 0.1911, 0.1827])\n",
        "train_transform=transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "    transforms.RandomErasing(scale=(0.02, 0.25))\n",
        "])\n",
        "test_transform=transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])"
      ],
      "metadata": {
        "id": "n6aUv7yYLNMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'DATASET'\n",
        "train_dataset = RAFDBDataset(root_dir=DATA_PATH, train=True, transform=train_transform)\n",
        "test_dataset = RAFDBDataset(root_dir=DATA_PATH, train=False, transform=test_transform)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "p9aXnCvnOIxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the data to check if the load was successful."
      ],
      "metadata": {
        "id": "Yofx1v39KIl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class_names=['surprise', 'fear', 'disgust', 'happy', 'sad', 'anger', 'natural']\n",
        "\n",
        "mean = np.array([0.5752, 0.4495, 0.4012])\n",
        "std = np.array([0.2086, 0.1911, 0.1827])\n",
        "\n",
        "def imshow_denormalize(axs, img, title):\n",
        "    img_denorm = img * std[:, None, None] + mean[:, None, None]\n",
        "    img_denorm = np.clip(img_denorm, 0, 1)\n",
        "    axs.imshow(np.transpose(img_denorm, (1, 2, 0)))\n",
        "    axs.set_title(title)\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "show_index = [0, 500, 1000, 2000]\n",
        "for i, index in enumerate(show_index):\n",
        "    img, label = test_dataset[index]\n",
        "    imshow_denormalize(axs[i], img, class_names[label])\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDH7Tyd1LOw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init Model"
      ],
      "metadata": {
        "id": "gi0ZQzKotcy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet-18 Model Structure\n",
        "\n",
        "For a comprehensive description of the model, you may refer to this paper: https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf"
      ],
      "metadata": {
        "id": "JiKoJtcJrYg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = torch.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_planes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 512 * BasicBlock.expansion)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def resnet18(pretrained=False, num_classes=7, pretrained_weights=None):\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
        "    if pretrained:\n",
        "      if pretrained_weights:\n",
        "        #Load the provided pretrained model weights\n",
        "        if os.path.isfile(pretrained_weights):\n",
        "          pretrain_dict = torch.load(pretrained_weights, map_location=torch.device('cpu'))\n",
        "        if 'state_dict' in pretrain_dict:\n",
        "          pretrain_dict = pretrain_dict['state_dict']\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in pretrain_dict.items() if k.replace('module.', '') in model.state_dict()}\n",
        "        state_dict.pop(\"fc.weight\")\n",
        "        state_dict.pop(\"fc.bias\")\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        print('Pretrained Weights Loaded')\n",
        "      else:\n",
        "        #Load the ImageNet pretrained model weights\n",
        "        pretrain_dict = model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth')\n",
        "        pretrain_dict.pop(\"fc.weight\")\n",
        "        pretrain_dict.pop(\"fc.bias\")\n",
        "        model.load_state_dict(pretrain_dict, strict=False)\n",
        "        print('Pretrained Weights Loaded')\n",
        "    return model"
      ],
      "metadata": {
        "id": "TucVuKrjbBcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(pretrained=True, num_classes=7).to(device)"
      ],
      "metadata": {
        "id": "EmavjpN2LogW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "LGZ4zCiEsSM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `criterion`: initializes a loss function, Cross-entropy loss is commonly used for classification tasks. It measures the performance of the model's predictions against the actual labels and is particularly useful when the target labels are mutually exclusive.\n",
        "* `optimizer`: creates an optimizer, which is an algorithm that adjusts the model's parameters to minimize the loss function. Here, the Adam optimizer is used.\n",
        "* `scheduler`: creates a learning rate scheduler, which is used to adjust the learning rate over time during training. The `ExponentialLR` scheduler reduces the learning rate by a factor of gamma every epoch, which can help the model converge more effectively."
      ],
      "metadata": {
        "id": "25HrM4QeaMjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
      ],
      "metadata": {
        "id": "UwF-WddQLpx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_epoch` function is designed to train a deep learning model for one epoch using a given training dataset. The function efficiently manages the training process, including forward and backward passes, parameter updates, and performance logging."
      ],
      "metadata": {
        "id": "Axg6-oLBselL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_dataloader, criterion, optimizer, epoch=0, log_eval=50):\n",
        "  accs, losses = [], []\n",
        "  start_time = time()\n",
        "  for idx, (X, y) in enumerate(train_dataloader):\n",
        "    start_time = time()\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = model(X)\n",
        "    loss = criterion(preds, y)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_acc = (preds.argmax(1) == y).sum().item()\n",
        "    acc = total_acc / y.size(0)\n",
        "    accs.append(acc)\n",
        "    end_iter_time = time() - start_time\n",
        "    if idx % log_eval == 0:\n",
        "      print(f\"Iteration: {idx} | time {end_iter_time} | train acc: {acc} | train_loss: {loss}\")\n",
        "  epoch_acc = sum(accs) / len(accs)\n",
        "  epoch_loss = sum(losses) / len(losses)\n",
        "  return epoch_acc, epoch_loss"
      ],
      "metadata": {
        "id": "dVA8sODPLrBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 20\n",
        "acc_list = []\n",
        "loss_list = []\n",
        "for epoch in tqdm(range(1, num_epoch+1)):\n",
        "  start_time = time()\n",
        "  train_acc, train_loss = train_epoch(model, train_dataloader, criterion, optimizer, epoch)\n",
        "  scheduler.step()\n",
        "  end_epoch_time = time() - start_time\n",
        "  print(f\"End of epoch {epoch} | time {end_epoch_time} | train acc: {train_acc}\\\n",
        "  train_loss: {train_loss}\")\n",
        "  acc_list.append(train_acc)\n",
        "  loss_list.append(train_loss)"
      ],
      "metadata": {
        "id": "8vg2QozQLyfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "acc_line, = ax1.plot(range(1, len(acc_list) + 1), acc_list, label='Accuracy', color='blue', marker='o')\n",
        "ax1.set_title('Training Accuracy')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "for i, acc in enumerate(acc_list):\n",
        "    ax1.text(i + 1, acc, f'{acc:.2f}', ha='center', va='bottom', fontsize=8)\n",
        "ax1.set_xticks(range(1, len(acc_list) + 1))\n",
        "ax1.legend()\n",
        "\n",
        "loss_line, = ax2.plot(range(1, len(loss_list) + 1), loss_list, label='Loss', color='red', marker='o')\n",
        "ax2.set_title('Training Loss')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Loss')\n",
        "for i, loss in enumerate(loss_list):\n",
        "    ax2.text(i + 1, loss, f'{loss:.2f}', ha='center', va='top', fontsize=8)\n",
        "ax2.set_xticks(range(1, len(loss_list) + 1))\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZENwvZKatK4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model"
      ],
      "metadata": {
        "id": "UKKpwxx7tmhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_epoch(model, test_dataloader):\n",
        "    accs, losses = [], []\n",
        "    for idx, (X, y) in enumerate(test_dataloader):\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        model.eval()\n",
        "        preds = model(X)\n",
        "\n",
        "        total_acc = (preds.argmax(1)==y).sum().item()\n",
        "        acc = total_acc / y.size(0)\n",
        "        accs.append(acc)\n",
        "    epoch_acc = sum(accs) / len(accs)\n",
        "    return epoch_acc"
      ],
      "metadata": {
        "id": "vVAcJb5KZXgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = eval_epoch(model, test_dataloader)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "rFxzPAaOZ_Rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer Learning"
      ],
      "metadata": {
        "id": "yC7WincRLBP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you may have observed, when we instantiate the ResNet-18 architecture, we utilize weights from a pre-trained model.**\n",
        "\n",
        "Pretrained models play a significant role in the field of deep learning. They are trained on large-scale datasets to learn general feature representations that can be transferred to other tasks, reducing the time and data required to train new models from scratch."
      ],
      "metadata": {
        "id": "qPA_mMCCLlYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will intuitively demonstrate the effects brought by pre-trained models through a practical attempt."
      ],
      "metadata": {
        "id": "cxt9dQ59boxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, download the ResNet-18 model weights pretrained with the Celeb-1M dataset, which are specifically optimized for face-related tasks."
      ],
      "metadata": {
        "id": "gVWI3VUuIe6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "gdown.download('https://drive.google.com/uc?id=1e7FmEfTIB__ATpSw5oHz61N1-bTl0Dlk', './resnet18_celeb.pth')"
      ],
      "metadata": {
        "id": "K6k2E-W_Iatx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we repeat the training process with the new model."
      ],
      "metadata": {
        "id": "eHyNPrx5dO0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = resnet18(pretrained=True, num_classes=7, pretrained_weights='./resnet18_celeb.pth').to(device)"
      ],
      "metadata": {
        "id": "4Kek0PYXO5xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.00005, weight_decay=1e-4)\n",
        "scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer2, gamma=0.9)"
      ],
      "metadata": {
        "id": "s0CHMlv4PFZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10\n",
        "acc_list2 = []\n",
        "loss_list2 = []\n",
        "for epoch in tqdm(range(1, num_epoch+1)):\n",
        "  start_time = time()\n",
        "  train_acc, train_loss = train_epoch(model2, train_dataloader, criterion, optimizer2, epoch)\n",
        "  scheduler2.step()\n",
        "  end_epoch_time = time() - start_time\n",
        "  print(f\"End of epoch {epoch} | time {end_epoch_time} | train acc: {train_acc}\\\n",
        "  train_loss: {train_loss}\")\n",
        "  acc_list2.append(train_acc)\n",
        "  loss_list2.append(train_loss)"
      ],
      "metadata": {
        "id": "jJhm3RiyPNnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "acc_line, = ax1.plot(range(1, len(acc_list2) + 1), acc_list2, label='Accuracy', color='blue', marker='o')\n",
        "ax1.set_title('Training Accuracy')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "for i, acc in enumerate(acc_list2):\n",
        "    ax1.text(i + 1, acc, f'{acc:.2f}', ha='center', va='bottom')\n",
        "ax1.set_xticks(range(1, len(acc_list2) + 1))\n",
        "ax1.legend()\n",
        "\n",
        "loss_line, = ax2.plot(range(1, len(loss_list2) + 1), loss_list2, label='Loss', color='red', marker='o')\n",
        "ax2.set_title('Training Loss')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Loss')\n",
        "for i, loss in enumerate(loss_list2):\n",
        "    ax2.text(i + 1, loss, f'{loss:.2f}', ha='center', va='top')\n",
        "ax2.set_xticks(range(1, len(loss_list2) + 1))\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wxr7BU36abGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.eval()\n",
        "test_acc2 = eval_epoch(model2, test_dataloader)\n",
        "print(test_acc2)"
      ],
      "metadata": {
        "id": "hUjMPbHZUcL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we can achieve a very large performance improvement just by changing the pre-trained model without making any other changes. This is because the features pre-trained on Celeb-1M are better suited to the task of facial expression recognition. Therefore, when training the model, it is very important to choose a suitable pre-training model weights."
      ],
      "metadata": {
        "id": "cGwf9yJrdU6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notice**"
      ],
      "metadata": {
        "id": "_XzQmC5xgtQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model weights using the following code and **move it to your Google Drive**. We will use this model in the next lab."
      ],
      "metadata": {
        "id": "xLh_ce6zhD2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2, 'fer_resnet18.pth')"
      ],
      "metadata": {
        "id": "bFLiABSrg_c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Test Result"
      ],
      "metadata": {
        "id": "VODf3XUQtpF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "class_names = ['surprise', 'fear', 'disgust', 'happy', 'sad', 'anger', 'natural']\n",
        "\n",
        "# Select the samples from the test set\n",
        "indices = [0, 400, 800, 1800]\n",
        "test_images = [test_dataset[i][0] for i in indices]\n",
        "labels = [test_dataset[i][1] for i in indices]\n",
        "\n",
        "mean = np.array([0.5752, 0.4495, 0.4012])\n",
        "std = np.array([0.2086, 0.1911, 0.1827])\n",
        "\n",
        "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "for i, (test_image, label) in enumerate(zip(test_images, labels)):\n",
        "    img_denorm = test_image * std[:, None, None] + mean[:, None, None]\n",
        "    img_denorm = np.clip(img_denorm, 0, 1)\n",
        "\n",
        "    axs[i].imshow(np.transpose(img_denorm, (1, 2, 0)))\n",
        "    axs[i].set_title(class_names[label])\n",
        "    axs[i].axis('off')\n",
        "\n",
        "    image = test_image.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)\n",
        "        pred = model(image)\n",
        "        predicted_class = pred.argmax(1).item()\n",
        "\n",
        "    print(f\"Image {indices[i]} - True label: {class_names[label]}, Predicted label: {class_names[predicted_class]}\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_68V0SJXLM5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Data Test\n",
        "\n",
        "Replace the path with your own data to test the result."
      ],
      "metadata": {
        "id": "D3ARaWB3uDLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=['surprise', 'fear', 'disgust', 'happy', 'sad', 'anger', 'natural']\n",
        "\n",
        "test_img_path = 'PATH_TO_YOUR_DATA'\n",
        "test_image = Image.open(test_img_path).convert('RGB')\n",
        "test_image = test_transform(test_image)\n",
        "\n",
        "mean = np.array([0.5752, 0.4495, 0.4012])\n",
        "std = np.array([0.2086, 0.1911, 0.1827])\n",
        "img_denorm = test_image * std[:, None, None] + mean[:, None, None]\n",
        "img_denorm = np.clip(img_denorm, 0, 1)\n",
        "\n",
        "plt.imshow(np.transpose(img_denorm, (1, 2, 0)))\n",
        "\n",
        "image = test_image.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image = image.unsqueeze(0)\n",
        "    pred = model(image)\n",
        "    predicted_class = pred.argmax(1).item()\n",
        "\n",
        "print(predicted_class)\n",
        "print(f\"Predicted class name: {class_names[predicted_class]}\")"
      ],
      "metadata": {
        "id": "leT82BudqOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**"
      ],
      "metadata": {
        "id": "5wMlFEHmebse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've learned from the case study, it's your turn to try training a network yourself"
      ],
      "metadata": {
        "id": "sMsnZoZnefYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using what you learned from the case study, try training with the following dataset: https://www.kaggle.com/datasets/zawarkhan69/human-facial-expression-dataset\n",
        "\n",
        "Instructions\n",
        "\n",
        "* Download and visualize statistics about the dataset\n",
        "* Create a suitable dataloader for the dataset\n",
        "* Set a proper `transform` and training hyperparameters\n",
        "* Train and evaluate the model"
      ],
      "metadata": {
        "id": "ypr5zcM9ewBx"
      }
    }
  ]
}