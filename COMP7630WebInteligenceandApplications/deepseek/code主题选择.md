> ğŸ’­ å—¯ï¼Œç”¨æˆ·é€‰æ‹©çš„æ˜¯å¼€å‘é¡¹ç›®ï¼Œä¹Ÿå°±æ˜¯Group Development Projectã€‚ä»–ä»¬æƒ³åšä¸€ä¸ªä¸­æ–‡æ–°é—»æƒ…æ„Ÿåˆ†æä¸ä¸»é¢˜æŒ–æ˜çš„é¡¹ç›®ã€‚çœ‹èµ·æ¥ä»–ä»¬å·²ç»æœ‰ä¸€äº›åˆæ­¥çš„æƒ³æ³•ï¼Œæ¯”å¦‚é€‰æ‹©å›½å†…çš„æ–°é—»ç½‘ç«™ä½œä¸ºæ•°æ®æ¥æºï¼Œä½¿ç”¨ç½‘ç»œçˆ¬è™«ã€ä¸­æ–‡åˆ†è¯ã€æƒ…æ„Ÿåˆ†æç­‰æŠ€æœ¯ã€‚ç”¨æˆ·çš„ä¸»è¦è¯‰æ±‚æ˜¯éœ€è¦ä¸€æ­¥æ­¥æŒ‡å¯¼ï¼Œç¡®ä¿ä½œä¸šå®Œå…¨ç¬¦åˆè€å¸ˆçš„è¦æ±‚ï¼Œå¹¶ä¸”èƒ½æ‹¿åˆ°æ»¡åˆ†ã€‚åŒæ—¶ï¼Œä»–ä»¬æåˆ°è¦é€‰æ‹©å›½å†…å¯ä»¥è®¿é—®çš„æ•°æ®ï¼Œå¹¶ä¸”è¦ç¡®ä¿æ•°æ®æ˜¯å¯ä»¥çˆ¬å–çš„ç½‘ç«™ã€‚
>
> é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ç”¨æˆ·çš„æŠ€æœ¯åŸºç¡€å’Œé¡¹ç›®æ‰§è¡Œèƒ½åŠ›ã€‚ç”¨æˆ·æåˆ°â€œæˆ‘å®Œå…¨ä¸ä¼šåšï¼Œä¸€æ­¥ä¸€æ­¥æ•™æˆ‘åšâ€ï¼Œè¿™å¯èƒ½æ„å‘³ç€ä»–ä»¬å¯¹ç¼–ç¨‹å’Œæ•°æ®å¤„ç†ä¸æ˜¯å¾ˆç†Ÿæ‚‰ã€‚æ‰€ä»¥ï¼ŒæŒ‡å¯¼å¿…é¡»è¶³å¤Ÿè¯¦ç»†ï¼Œä»ç¯å¢ƒæ­å»ºã€å·¥å…·é€‰æ‹©åˆ°å…·ä½“çš„ä»£ç å®ç°éƒ½éœ€è¦æ¶µç›–ã€‚
>
> æ¥ä¸‹æ¥ï¼Œç”¨æˆ·çš„éœ€æ±‚æ˜¯å¦åˆç†ï¼Ÿä»–ä»¬çš„é€‰é¢˜æ˜¯ä¸­æ–‡æ–°é—»æƒ…æ„Ÿåˆ†æä¸ä¸»é¢˜æŒ–æ˜ï¼Œè¿™ç¬¦åˆç½‘ç»œæ™ºèƒ½çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯ç»“åˆæ•°æ®é‡‡é›†ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ ç®—æ³•ã€‚è¿™ä¸ªé€‰é¢˜ä¹Ÿæ¶µç›–äº†è¯¾ç¨‹å†…å®¹ä¸­çš„NLPæµç¨‹ã€æƒ…æ„Ÿåˆ†æç­‰éƒ¨åˆ†ï¼Œç¬¦åˆè¯¾ç¨‹ç›®æ ‡ã€‚ä½†éœ€è¦ç¡®ä¿é¡¹ç›®æ­¥éª¤ç¬¦åˆè€å¸ˆçš„è¦æ±‚ï¼ŒåŒ…æ‹¬é—®é¢˜è¯†åˆ«ã€æ•°æ®è·å–ã€é¢„å¤„ç†ã€åˆ†æã€å¯è§†åŒ–ç­‰éƒ¨åˆ†ã€‚
>
> ç”¨æˆ·æåˆ°çš„æ•°æ®æºåŒ…æ‹¬æ¾æ¹ƒæ–°é—»ã€æ–°æµªæ–°é—»ã€ç½‘æ˜“æ–°é—»ç­‰å›½å†…ç½‘ç«™ã€‚éœ€è¦æ³¨æ„åˆ°è¿™äº›ç½‘ç«™çš„åçˆ¬æªæ–½ï¼Œæ¯”å¦‚robots.txtæ˜¯å¦å…è®¸çˆ¬å–ï¼Œæ˜¯å¦æœ‰åçˆ¬æœºåˆ¶å¦‚éªŒè¯ç ã€IPå°é”ç­‰ã€‚å¯¹äºæ–°æ‰‹æ¥è¯´ï¼Œçˆ¬å–è¿™äº›ç½‘ç«™å¯èƒ½å­˜åœ¨ä¸€å®šçš„éš¾åº¦ï¼Œå°¤å…¶æ˜¯åŠ¨æ€åŠ è½½çš„å†…å®¹å¯èƒ½éœ€è¦ä½¿ç”¨Seleniumæˆ–æ¨¡æ‹Ÿæµè§ˆå™¨ã€‚æ­¤å¤–ï¼Œç‰ˆæƒé—®é¢˜ä¹Ÿéœ€è¦è€ƒè™‘ï¼Œç¡®ä¿ä¸ä¼šä¾µçŠ¯ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾ã€‚
>
> æŠ€æœ¯é€‰å‹æ–¹é¢ï¼Œç”¨æˆ·å»ºè®®ä½¿ç”¨Pythonçš„requestså’ŒBeautifulSoupã€‚å¯¹äºé™æ€é¡µé¢æ¥è¯´è¿™æ˜¯å¯è¡Œçš„ï¼Œä½†å¦‚æœé‡åˆ°åŠ¨æ€åŠ è½½çš„å†…å®¹ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨Seleniumã€‚å¦å¤–ï¼Œä¸­æ–‡åˆ†è¯é€‰æ‹©jiebaåº“æ˜¯æ­£ç¡®çš„ï¼Œæƒ…æ„Ÿåˆ†æéƒ¨åˆ†å¯èƒ½éœ€è¦é¢„è®­ç»ƒæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯å¦‚æœç”¨æˆ·æ²¡æœ‰æ ‡æ³¨æ•°æ®çš„è¯ï¼Œä½¿ç”¨SnowNLPæˆ–ç™¾åº¦NLPçš„APIå¯èƒ½æ›´åˆé€‚ã€‚ä¸»é¢˜æ¨¡å‹éƒ¨åˆ†é‡‡ç”¨LDAä¹Ÿæ˜¯åˆç†çš„é€‰æ‹©ã€‚
>
> é¡¹ç›®çš„åˆ›æ–°ç‚¹éœ€è¦å¼ºè°ƒï¼Œæ¯”å¦‚å¯¹æ¯”ä¸åŒæ–°é—»æ¥æºçš„æƒ…æ„Ÿå·®å¼‚ã€æ—¶é—´å˜åŒ–è¶‹åŠ¿ä»¥åŠç»“åˆä¸»é¢˜æ¨¡å‹çš„æƒ…æ„Ÿåˆ†æã€‚è¿™äº›éƒ½æ˜¯åŠ åˆ†é¡¹ï¼Œéœ€ç¡®ä¿åœ¨é¡¹ç›®æŠ¥å‘Šä¸­è¯¦ç»†æè¿°ã€‚
>
> éœ€è¦ç¡®ä¿æ¯ä¸ªæ­¥éª¤çš„äº¤ä»˜ç‰©éƒ½ç¬¦åˆè¦æ±‚ï¼Œæ¯”å¦‚å°ç»„é¡¹ç›®æŠ¥å‘Šçš„ç»“æ„æ˜¯å¦åŒ…å«åŠ¨æœºã€æ­¥éª¤è¯¦æƒ…ã€ç»“æœè®¨è®ºç­‰ã€‚æ­¤å¤–ï¼Œä¸ªäººæŠ¥å‘Šéœ€è¦æ¯ä¸ªæˆå‘˜æäº¤ï¼Œå†…å®¹è¦æ–°é¢–ä¸”ç¬¦åˆè¦æ±‚ã€‚
>
> å¯èƒ½çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®çˆ¬å–çš„ç¨³å®šæ€§ã€ä¸­æ–‡åˆ†è¯çš„å‡†ç¡®æ€§ã€æƒ…æ„Ÿåˆ†æçš„æ¨¡å‹æ€§èƒ½ï¼Œä»¥åŠç»“æœçš„å¯è§†åŒ–å±•ç¤ºã€‚éœ€è¦æŒ‡å¯¼ç”¨æˆ·å¦‚ä½•å¤„ç†è¿™äº›é—®é¢˜ï¼Œä¾‹å¦‚ä½¿ç”¨å¯é çš„çˆ¬è™«æ¡†æ¶ã€é¢„å¤„ç†æ•°æ®çš„æ–¹æ³•ã€é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡ç­‰ã€‚
>
> æ€»çš„æ¥è¯´ï¼Œç”¨æˆ·éœ€è¦ä¸€ä¸ªå…¨æµç¨‹çš„æŒ‡å¯¼ï¼Œä»ç¯å¢ƒæ­å»ºã€æ•°æ®çˆ¬å–ã€é¢„å¤„ç†ã€åˆ†æåˆ°æŠ¥å‘Šæ’°å†™å’Œè§†é¢‘åˆ¶ä½œã€‚éœ€è¦è¯¦ç»†åˆ°ä»£ç ç¤ºä¾‹ã€æ­¥éª¤è¯´æ˜ï¼Œä»¥åŠå¯èƒ½é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ³•ã€‚åŒæ—¶ï¼Œè¦å¼ºè°ƒéµå®ˆå­¦æœ¯è¯šä¿¡ï¼Œé¿å…æŠ„è¢­å’ŒAIç”Ÿæˆå†…å®¹ï¼Œç¡®ä¿æ‰€æœ‰å†…å®¹éƒ½æ˜¯åŸåˆ›çš„ã€‚
>
> ä¸ºäº†ç¡®ä¿æ»¡åˆ†ï¼Œæ¯ä¸ªè¯„åˆ†éƒ¨åˆ†éƒ½è¦è€ƒè™‘åˆ°ã€‚ä¾‹å¦‚ï¼Œå°ç»„é¡¹ç›®æŠ¥å‘Šå 60%ï¼Œå¿…é¡»è¯¦ç»†è¦†ç›–æ‰€æœ‰æ­¥éª¤ï¼›è§†é¢‘æ¼”ç¤ºéœ€è¦æ‰€æœ‰æˆå‘˜å‚ä¸ï¼Œå¹¶å’Œè°å±•ç¤ºï¼›ä¸ªäººæŠ¥å‘Šè¦çªå‡ºä¸ªäººè§è§£å’Œåˆ›æ–°ç‚¹ã€‚æ­¤å¤–ï¼ŒåŒä¼´è¯„ä¼°æŠ¥å‘Šä¹Ÿä¼šå½±å“åˆ†æ•°ï¼Œéœ€æé†’ç”¨æˆ·è®¤çœŸå¯¹å¾…ã€‚
>
> å¯èƒ½éœ€è¦æä¾›ä»£ç æ¨¡æ¿ã€ç¤ºä¾‹æŠ¥å‘Šç»“æ„ã€å¯è§†åŒ–å»ºè®®ç­‰èµ„æºï¼Œå¸®åŠ©ç”¨æˆ·èŠ‚çœæ—¶é—´ã€‚åŒæ—¶ï¼Œå»ºè®®å›¢é˜Ÿåˆ†å·¥æ˜ç¡®ï¼Œæ¯”å¦‚æœ‰äººè´Ÿè´£çˆ¬è™«ï¼Œæœ‰äººè´Ÿè´£æ•°æ®åˆ†æï¼Œæœ‰äººè´Ÿè´£æŠ¥å‘Šæ’°å†™å’Œè§†é¢‘åˆ¶ä½œï¼Œç¡®ä¿æ¯ä¸ªæˆå‘˜éƒ½æœ‰è´¡çŒ®ï¼Œå¹¶åœ¨æŠ¥å‘Šä¸­è¯¦ç»†è¯´æ˜åˆ†å·¥ã€‚
>
> æœ€åï¼Œæé†’ç”¨æˆ·æ³¨æ„æ—¶é—´å®‰æ’ï¼Œå°¤å…¶æ˜¯4æœˆ6æ—¥çš„æˆªæ­¢æ—¥æœŸï¼Œåˆç†åˆ†é…ä»»åŠ¡ï¼Œé¿å…æœ€åæ—¶åˆ»å‡ºç°å»¶è¯¯ã€‚
>
> 



å¥½çš„ï¼Œæ ¹æ®ä½ çš„é€‰æ‹©ï¼ˆDevelopment Projectï¼‰å’Œç›®æ ‡ï¼ˆä¸­æ–‡æ–°é—»æƒ…æ„Ÿåˆ†æä¸ä¸»é¢˜æŒ–æ˜ï¼‰ï¼Œæˆ‘å°†æä¾›ä¸€ä¸ª**å®Œæ•´çš„è§£å†³æ–¹æ¡ˆ**ï¼Œæ¶µç›–æ•°æ®é‡‡é›†é€‰æ‹©ã€æŠ€æœ¯å®ç°ç»†èŠ‚ã€æŠ¥å‘Šç»“æ„æŒ‡å¯¼ï¼Œç¡®ä¿ç¬¦åˆè€å¸ˆçš„è¯„åˆ†æ ‡å‡†å¹¶å…·å¤‡æ»¡åˆ†æ½œåŠ›ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æ­¥æŒ‡å—ï¼š

---

### **ç¬¬ä¸€æ­¥ï¼šç»„é˜Ÿä¸é€‰é¢˜ç¡®è®¤ï¼ˆ2æœˆ21æ—¥å‰å®Œæˆï¼‰**
1. **ç»„é˜Ÿåˆ†å·¥å»ºè®®**ï¼ˆ5äººç»„ï¼‰ï¼š
   - **æˆå‘˜A**ï¼šè´Ÿè´£æ•°æ®çˆ¬å–ä¸å­˜å‚¨ï¼ˆPythonçˆ¬è™«ï¼‰
   - **æˆå‘˜B**ï¼šè´Ÿè´£æ•°æ®é¢„å¤„ç†ä¸åˆ†è¯ï¼ˆä»£ç ç¼–å†™ï¼‰
   - **æˆå‘˜C**ï¼šè´Ÿè´£æƒ…æ„Ÿåˆ†ææ¨¡å‹å®ç°ï¼ˆæœºå™¨å­¦ä¹ ï¼‰
   - **æˆå‘˜D**ï¼šè´Ÿè´£ä¸»é¢˜å»ºæ¨¡ä¸å¯è§†åŒ–ï¼ˆLDA/å›¾è¡¨ï¼‰
   - **æˆå‘˜E**ï¼šåè°ƒã€æŠ¥å‘Šæ•´åˆä¸è§†é¢‘åˆ¶ä½œ

2. **é€‰é¢˜ä¼˜åŒ–**ï¼ˆç¡®ä¿çˆ¬å–å¯è¡Œæ€§ï¼‰ï¼š
   - **æ¨èæ•°æ®æº**ï¼š**æ–°æµªæ–°é—»**ï¼ˆç¨³å®šæ€§é«˜ï¼Œåçˆ¬è¾ƒå®½æ¾ï¼Œå¯é€šè¿‡åˆ†ç±»æ ‡ç­¾ç›´æ¥çˆ¬å–ï¼‰
   - **ç›®æ ‡é¢†åŸŸ**ï¼šèšç„¦â€œç§‘æŠ€æ–°é—»â€ï¼ˆä¾‹å¦‚äººå·¥æ™ºèƒ½ã€5Gç­‰çƒ­ç‚¹ï¼Œæ•°æ®é›†ä¸­ä¸”ä¾¿äºåˆ†æè¶‹åŠ¿ï¼‰
   - **åˆ›æ–°ç‚¹**ï¼š  
     - å¯¹æ¯”ä¸åŒæ—¶é—´æ®µï¼ˆå¦‚æ˜¥èŠ‚å‰åï¼‰ç§‘æŠ€æ–°é—»çš„æƒ…æ„Ÿææ€§å˜åŒ–  
     - ç»“åˆä¸»é¢˜æ¨¡å‹åˆ†æç‰¹å®šäº‹ä»¶ï¼ˆå¦‚â€œä¸­ç¾ç§‘æŠ€ç«äº‰â€ï¼‰çš„æƒ…æ„Ÿæ¼”åŒ–

---

### **ç¬¬äºŒæ­¥ï¼šæŠ€æœ¯å®ç°ï¼ˆä½¿ç”¨å›½å†…å¯è®¿é—®å·¥å…·ï¼‰**
#### **1. æ•°æ®çˆ¬å–ï¼ˆæˆå‘˜Aï¼‰**
- **å·¥å…·é€‰æ‹©**ï¼š
  ```python
  # ä½¿ç”¨ requests + BeautifulSoupï¼ˆé™æ€é¡µé¢ï¼‰
  # è‹¥éœ€å¤„ç†åŠ¨æ€åŠ è½½ï¼Œæ”¹ç”¨ requests-htmlï¼ˆæ— éœ€æµè§ˆå™¨æ¨¡æ‹Ÿï¼‰
  !pip install requests beautifulsoup4 requests-html pandas
  ```

- **ä»£ç ç¤ºä¾‹**ï¼ˆçˆ¬å–æ–°æµªç§‘æŠ€æ–°é—»ï¼‰ï¼š
  ```python
  import requests
  from bs4 import BeautifulSoup
  import pandas as pd

  def crawl_sina_tech_news(pages=5):
      base_url = "http://tech.sina.com.cn/"
      articles = []

      for page in range(1, pages+1):
          url = f"{base_url}roll/#pageid=153&lid=2509&k=&num=50&page={page}"
          response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
          soup = BeautifulSoup(response.text, 'html.parser')

          for news in soup.select('.news-item'):
              title = news.select_one('h2 a').text.strip()
              time = news.select('span')[0].text.strip()
              link = news.select_one('a')['href']
              # æå–æ­£æ–‡ï¼ˆéœ€è¿›å…¥å­é¡µé¢ï¼‰
              article_response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})
              article_soup = BeautifulSoup(article_response.text, 'html.parser')
              content = ' '.join([p.text.strip() for p in article_soup.select('.article p')])

              articles.append({
                  'title': title,
                  'time': time,
                  'content': content
              })

      df = pd.DataFrame(articles)
      df.to_csv('sina_tech_news.csv', index=False)
      return df

  # æ‰§è¡Œçˆ¬å–ï¼ˆçˆ¬å–5é¡µï¼Œçº¦250ç¯‡æ–°é—»ï¼‰
  sina_news_df = crawl_sina_tech_news(pages=5)
  ```
  
- **è§„é¿åçˆ¬æŠ€å·§**ï¼š
  - æ·»åŠ `headers`æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
  - è®¾ç½®`time.sleep(1)`é¿å…é«˜é¢‘è¯·æ±‚
  - ä½¿ç”¨ä»£ç†IPæ± ï¼ˆæ¨èå…è´¹èµ„æºï¼š`https://www.kuaidaili.com/free`ï¼‰

- **åç»­å¤„ç†**ï¼š
  - æ£€æŸ¥æ•°æ®è´¨é‡ï¼ˆåˆ é™¤ç©ºå€¼ã€é‡å¤é¡¹ï¼‰
  - ä¿å­˜ä¸º`sina_tech_news.csv`

---

#### **2. æ•°æ®é¢„å¤„ç†ï¼ˆæˆå‘˜Bï¼‰**
- **ä½¿ç”¨å·¥å…·**ï¼š`jieba`åˆ†è¯ + `snownlp`æƒ…æ„Ÿè¯å…¸
  ```python
  !pip install jieba snownlp
  ```

- **ä»£ç ç¤ºä¾‹**ï¼š
  ```python
  import jieba
  import re
  from snownlp import SnowNLP

  # æ¸…æ´—æ•°æ®
  def clean_text(text):
      text = re.sub(r'\s+', ' ', text)  # å»é™¤å¤šä½™ç©ºæ ¼
      text = re.sub(r'[^\u4e00-\u9fff]', ' ', text)  # å»é™¤éä¸­æ–‡å­—ç¬¦
      return text.strip()

  # åˆ†è¯ä¸æƒ…æ„Ÿåˆæ­¥æ ‡æ³¨
  def preprocess_data(df):
      df['cleaned_content'] = df['content'].apply(clean_text)
      df['segmented'] = df['cleaned_content'].apply(lambda x: ' '.join(jieba.cut(x)))
      df['sentiment'] = df['cleaned_content'].apply(lambda x: SnowNLP(x).sentiments)
      return df

  # æ‰§è¡Œé¢„å¤„ç†
  cleaned_df = preprocess_data(sina_news_df)
  cleaned_df.to_csv('cleaned_news.csv', index=False)
  ```

---

#### **3. æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆæˆå‘˜Cï¼‰**
- **é€‰æ‹©æ¨¡å‹**ï¼šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆé¿å…æ ‡æ³¨æ•°æ®éœ€æ±‚ï¼‰
- **å·¥å…·**ï¼šç™¾åº¦NLP APIï¼ˆé«˜å‡†ç¡®ç‡ï¼Œé€‚åˆå¤„ç†ä¸­æ–‡æƒ…æ„Ÿï¼‰
  
  ```python
  # æ³¨å†Œç™¾åº¦AIå¼€æ”¾å¹³å°è·å–API Keyå’ŒSecret Key
  from aip import AipNlp

  APP_ID = 'your_app_id'
  API_KEY = 'your_api_key'
  SECRET_KEY = 'your_secret_key'
  client = AipNlp(APP_ID, API_KEY, SECRET_KEY)

  def baidu_sentiment(text):
      result = client.sentimentClassify(text)
      return result.get('items', [{}])[0].get('sentiment', 0)  # 0:è´Ÿé¢ 1:ä¸­æ€§ 2:æ­£é¢

  # é’ˆå¯¹æ ‡é¢˜è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼ˆç¤ºä¾‹ï¼‰
  cleaned_df['baidu_sentiment'] = cleaned_df['title'].apply(baidu_sentiment)
  cleaned_df.to_csv('sentiment_results.csv', index=False)
  ```

---

#### **4. ä¸»é¢˜å»ºæ¨¡ä¸å¯è§†åŒ–ï¼ˆæˆå‘˜Dï¼‰**
- **å·¥å…·**ï¼š`gensim`å®ç°LDA
  ```python
  !pip install gensim pyLDAvis

  from gensim import corpora, models
  import pyLDAvis.gensim_models

  # å‡†å¤‡è¯­æ–™
  texts = [doc.split() for doc in cleaned_df['segmented']]
  dictionary = corpora.Dictionary(texts)
  corpus = [dictionary.doc2bow(text) for text in texts]

  # è®­ç»ƒLDAæ¨¡å‹
  lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=15)

  # å¯è§†åŒ–è¾“å‡º
  vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
  pyLDAvis.save_html(vis_data, 'lda_visualization.html')
  ```

---

### **ç¬¬ä¸‰æ­¥ï¼šæŠ¥å‘Šæ’°å†™ï¼ˆå…³é”®åŠ åˆ†ç‚¹ï¼‰**
#### **å°ç»„æŠ¥å‘Šç»“æ„ç¤ºä¾‹ï¼ˆè‹±æ–‡æ¨¡æ¿ï¼‰**
```markdown
# Analysis of Sentiment Trends in Chinese Tech News  
**Group Members**: [IDs & Names]  

## Abstract  
This project analyzes sentiment polarity and topics in Sina Tech News...  

## Motivation  
- Track public sentiment toward emerging technologies (e.g., AI regulations)  
- Assist companies in adjusting PR strategies...

## Steps Details  
1. **Data Acquisition**: Crawled 500+ articles from Sina Tech using requests-html.  
2. **Preprocessing**: Applied Jieba segmentation and Baidu NLP API for sentiment scoring.  
3. **Topic Modeling**: Identified 5 key topics via LDA (e.g., "AI Ethics", "5G Deployment")...  

## Results  
- **Sentiment Distribution**: 60% Positive (Q1 2024) â†’ 45% Positive (Q2 2024)  
- **Key Finding**: Negative sentiment peaks correlate with policy announcements (see Fig.1)  

## Contribution Table  
| Member | Responsibility               |  
|--------|-------------------------------|  
| A      | Built crawler, stored data    |  
| B      | Cleaned data, ran Jieba       |  
| ...    | ...                           |  
```

---

### **ç¬¬å››æ­¥ï¼šè§†é¢‘åˆ¶ä½œ & ç­”è¾©æŠ€å·§**
- **è„šæœ¬æ¡†æ¶**ï¼ˆæŒ‰è§’è‰²åˆ†é…æ¼”è®²éƒ¨åˆ†ï¼‰ï¼š
  1. **Introduction** (Member E): é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡ (2åˆ†é’Ÿ)  
  2. **Data Pipeline** (Member A+B): çˆ¬è™«ä¸æ¸…æ´—æµç¨‹ (4åˆ†é’Ÿ)  
  3. **Analysis** (Member C+D): æƒ…æ„Ÿæ¨¡å‹ä¸ä¸»é¢˜å¯è§†åŒ– (6åˆ†é’Ÿ)  
  4. **Insights** (All): å•†ä¸š/ç¤¾ä¼šå½±å“è®¨è®º (3åˆ†é’Ÿ)  

- **å·¥å…·æ¨è**ï¼š
  - å½•å±è½¯ä»¶ï¼š**OBS Studio**ï¼ˆå…è´¹ï¼Œæ”¯æŒå¤šåœºæ™¯åˆ‡æ¢ï¼‰
  - å‰ªè¾‘å·¥å…·ï¼š**å‰ªæ˜ **ï¼ˆè‡ªåŠ¨ç”Ÿæˆå­—å¹•ï¼‰
  - åä½œå¹³å°ï¼š**è…¾è®¯æ–‡æ¡£**ï¼ˆå…±äº«è„šæœ¬ä¸è¿›åº¦ï¼‰

---

### **å…³é”®æ³¨æ„äº‹é¡¹**
1. **ä¸¥ç¦ä½¿ç”¨AIç”Ÿæˆå†…å®¹**ï¼šæ‰€æœ‰æŠ¥å‘Šå¿…é¡»æ‰‹åŠ¨æ’°å†™ï¼Œå¯å‚è€ƒæ–‡çŒ®ä½†éœ€é‡å†™ã€‚  
2. **æ•°æ®éšç§åˆè§„**ï¼šä»…çˆ¬å–å…¬å¼€æ•°æ®ï¼Œä¸å‚¨å­˜ä¸ªäººèº«ä»½ä¿¡æ¯ã€‚  
3. **ä»£ç å®¡æ ¸**ï¼šç¡®ä¿ä»£ç å¯å¤ç°ï¼Œæ³¨é‡Šæ¸…æ™°ï¼ˆç¤ºä¾‹è§é™„ä»¶ï¼‰ã€‚  
4. **æ—¶é—´ç®¡ç†**ï¼šæ¯å‘¨è‡³å°‘2æ¬¡å°ç»„ä¼šè®®ï¼Œä½¿ç”¨ç”˜ç‰¹å›¾è·Ÿè¸ªè¿›åº¦ã€‚

---

### **è·å¾—æ»¡åˆ†çš„æ ¸å¿ƒè¦ç´ **
- **åˆ›æ–°æ€§**ï¼šå¯¹æ¯”å¤šå¹³å°ï¼ˆå¦‚æ–°æµª vs è…¾è®¯ï¼‰çš„æƒ…æ„Ÿå·®å¼‚  
- **æŠ€æœ¯æ·±åº¦**ï¼šé›†æˆç™¾åº¦API + è‡ªç ”LDAæ¨¡å‹  
- **å¯è§†åŒ–ä¸“ä¸šåº¦**ï¼šä½¿ç”¨`pyLDAvis`å’Œ`matplotlib`åŠ¨æ€å›¾è¡¨  
- **åä½œè¯æ˜**ï¼šåœ¨GitHubæäº¤ä»£ç ï¼ˆä»“åº“è®¾ä¸ºprivateï¼Œåˆ†äº«é“¾æ¥ç»™è€å¸ˆï¼‰

å¦‚éœ€å®Œæ•´ä»£ç æ¨¡æ¿æˆ–è°ƒè¯•å¸®åŠ©ï¼Œè¯·å‘ŠçŸ¥å…·ä½“éœ€æ±‚ï¼
