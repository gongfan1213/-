COMP7630 - 网络智能及其应用
COMP7630 – Web Intelligence and its Applications

基本概率理论
Basic Probability Theory

瓦伦蒂诺·桑图奇
Valentino Santucci
（valentino.santucci@unistrapg.it）
(valentino.santucci@unistrapg.it)

多项分布
Multinomial Distribution
- 多项分布本质上是一个概率向量。
  - A multinomial distribution is essentially a probability vector.
- 示例：
  - Example:
  - fruits=['apple','banana',"organge",'mango']
  - probs=[0.3, 0.2, 0.1, 0.4]

从多项分布中采样
Sampling from a Multinomial Distribution
- 轮盘赌采样：
  - Roulette wheel sampling:
  - 创建与概率向量对应的累积分布；
    - create the cumulative distribution corresponding to the probability vector;
  - 生成一个在[0,1]区间内的随机数r；
    - generate a random number r in [0,1];
  - 在累积向量中找到小于r的最大项，该项的索引就是采样得到的索引。
    - find the largest entry in the cumulative vector which is smaller than r. It is the index sampled.

用纯Python和numpy实现的轮盘赌采样
Roulette Wheel Sampling in Python in plain Python using numpy
In [42]:import random
In[43]:
In [43]:#定义概率分布
  - In [44]:probs=[0.3, 0.2, 0.1, 0.4]
  - In[45]:fruits=['apple','banana',"orange",'mango']
  - In[46]:
  - In[46]:#创建累积分布
  - In [47]:cumulative_probs=[probs[0]]
  - In[48]:for i in range(1, len(probs)): cumulative_probs.append(cumulative_probs[i - 1]+probs[i])
  - In[49]:#生成一个0到1之间的随机数
  - In[50]:rand=random.random()
  - In[51]:
  - In[51]:#找到随机数落入的区间的索引
  - In[52]:index=0
  - In[53]:while rand>cumulative_probs[index]:
  - index += 1
  - In[54]:#根据采样得到的索引输出水果
  - In[55]:print(fruits[index])
  - mango
  - In [57]:import numpy as np
  - In[58]:
  - In[58]:#定义概率分布
  - In[59]:probs=[0.3, 0.2, 0.1, 0.4]
  - In[60]:fruits=['apple','banana',"orange",'mango']
  - In[61]:
  - In[61]:#创建累积分布
  - In[62]:cumulative_probs=np.cumsum(probs)
  - In[63]:
  - In[63]:#生成一个0到1之间的随机数
  - In[64]:rand=np.random.rand()
  - In[65]:
  - In[65]:#找到随机数落入的区间的索引
  - In[66]:index=np.searchsorted(cumulative_probs, rand)
  - In[67]:
  - In [67]:#根据采样得到的索引输出水果
  - In [68]:print(fruits[index])
  - mango

独立事件的联合概率
Joint Probability of Independent Events
- 假设\(E_{1}, E_{2}, ..., E_{n}\)是n个独立事件，它们发生的概率分别为\(p_{1}, p_{2}, ..., p_{n}\)。
  - Suppose \(E_{1}, E_{2}, ..., E_{n}\) are n independent events occurring with probabilities \(p_{1}, p_{2}, ..., p_{n}\).
- 那么，所有事件\(E_{1}, E_{2}, ..., E_{n}\)同时发生的联合概率为\(\prod_{i = 1}^{n} p_{i}\)。
  - Then, the joint probability that all the events \(E_{1}, E_{2}, ..., E_{n}\) occur altogether is \(\prod_{i = 1}^{n} p_{i}\).
- 在\(p = p_{1} = p_{2}=\cdots=p_{n}\)的情况下，那么（所有事件同时发生的）联合概率为\(p^{n}\)。
  - In the case \(p = p_{1} = p_{2}=\cdots=p_{n}\), then the joint probability (of all events occurring simultaneously) is \(p^{n}\).

条件概率
Conditional Probability
\[P(Y | X)=\frac{P(X, Y)}{P(X)}\]
\[P(Y | X)=\frac{P(X, Y)}{P(X)}\]
\[P(X, Y)=P(Y | X) P(X)=P(X | Y) P(Y)\]
\[P(X, Y)=P(Y | X) P(X)=P(X | Y) P(Y)\]

用于学习的贝叶斯推断
Bayes Inference for Learning
- 贝叶斯定理：
  - Bayes Theorem:
  - 其中：
  - where:
  \[p(H | D)=\frac{p(D | H) p(H)}{p(D)}\]
  \[p(H | D)=\frac{p(D | H) p(H)}{p(D)}\]
  - D通常指观测数据（即数据矩阵）
  - D usually refer to observed data (i.e. a data - matrix)
  - H通常指模型的参数（例如某些概率分布）。贝叶斯定理通常通过将后验作为下一次迭代的先验来迭代应用。这将允许迭代地提高模型对数据的拟合精度。
  - H usually refer to the parameters of a model (e.g. some probability distribution). Bayes theorem is usually applied iteratively by setting posterior as prior in the next iteration. This will allow to iteratively increase the accuracy of how the model fits to the data.
  - 当比较两个模型时，分母中的证据项可以省略（实际上，它仅取决于数据）。
  - When there are two models to compare, the evidence in the denominator can be omitted (in fact, it only depends from the data). 
