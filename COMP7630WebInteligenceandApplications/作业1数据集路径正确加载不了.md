你的问题仍然是 **NLTK punkt 资源丢失** 和 **数据集没有正确加载** 导致 `train_test_split()` 失败。让我们逐步解决问题。

---

## **第一步：修复 NLTK punkt 资源丢失**
Colab 可能有缓存问题，因此你需要强制删除 `nltk_data` 目录并重新下载 `punkt` 资源：

```python
!rm -rf /root/nltk_data  # 删除 NLTK 可能损坏的缓存
import nltk
nltk.download('punkt')  # 重新下载 punkt 资源
```
然后**重启 Colab 运行时**（点击 `Runtime` -> `Restart runtime`）。

---

## **第二步：检查数据集是否正确加载**
你的错误信息 `Total samples loaded: 0` 表明 **没有成功加载任何 txt 文件**。请运行以下代码检查数据集路径是否正确：

```python
import os

# 你的数据集路径
data_dir = "/content/drive/MyDrive/dataset/dataset"

# 确保数据集目录存在
if os.path.exists(data_dir):
    print("✅ 数据集目录存在")
else:
    print("❌ 数据集目录不存在，请检查路径")

# 列出 dataset 目录中的文件夹
print("📂 dataset 目录内容：", os.listdir(data_dir))

# 检查子文件夹是否存在
categories = ['cakes', 'chocolate_bars', 'cookies', 'soft_drinks', 'teas']
for category in categories:
    category_path = os.path.join(data_dir, category)
    if os.path.exists(category_path):
        print(f"✅ 发现类别文件夹：{category}")
        print("📜 文件列表（前5个）：", os.listdir(category_path)[:5])
    else:
        print(f"❌ 未找到类别文件夹：{category}")
```
#### **可能的情况**
1. **如果 `❌ 数据集目录不存在`，说明 `data_dir` 路径错误**  
   - **解决方案：** 手动检查 Google Drive 的实际路径，并更新 `data_dir`。  
   - 你可以在 Colab 运行 `!ls /content/drive/MyDrive/` 找到正确的路径。

2. **如果 `❌ 未找到类别文件夹`，说明 `dataset` 目录结构错误**  
   - 可能你 **解压后少了一层目录**，应该进入 `dataset/dataset/` 而不是 `dataset/`。

3. **如果 `✅ 数据集目录存在` 并列出了 `txt` 文件，但 `Total samples loaded: 0`**  
   - 可能是 `glob.glob(os.path.join(folder, '*.txt'))` 失败，尝试手动列出 `txt` 文件：
   ```python
   for category in categories:
       folder = os.path.join(data_dir, category)
       txt_files = glob.glob(os.path.join(folder, '*.txt'))
       if txt_files:
           print(f"✅ {category} 目录中找到 {len(txt_files)} 个 txt 文件")
       else:
           print(f"❌ {category} 目录中没有 txt 文件")
   ```

---

## **第三步：调试数据加载**
如果 **数据目录正确但 `Total samples loaded: 0`**，你需要 **单独测试 `txt` 文件是否能正确读取**：

```python
import glob

# 选取某个类别的文件夹
test_category = "teas"  # 你可以换成 "cakes", "cookies" 等
test_folder = os.path.join(data_dir, test_category)

# 获取文件列表
txt_files = glob.glob(os.path.join(test_folder, '*.txt'))

# 读取前2个文件的内容
for txt_file in txt_files[:2]:
    try:
        with open(txt_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"\n📜 读取文件: {txt_file}")
            print("🔹 第一行（标题）：", lines[0].strip() if lines else "空文件")
            print("🔹 总行数：", len(lines))
    except Exception as e:
        print(f"❌ 读取 {txt_file} 失败：{e}")
```
#### **可能的情况**
1. **如果 `txt_files=[]`，说明 `glob.glob()` 找不到 `txt` 文件**  
   - 可能 `*.txt` 匹配模式有误，尝试 `print(os.listdir(test_folder))` 检查实际文件名。

2. **如果 `读取文件失败`，说明某些 `txt` 文件编码有问题**  
   - 可能 `utf-8` 解析失败，可以尝试 `encoding="ISO-8859-1"` 或 `encoding="utf-16"`。

---

## **第四步：确认数据集正确后，重新运行代码**
如果上述检查 **所有都正常**，但仍然 `Total samples loaded: 0`，你可以运行完整的数据加载代码：

```python
import nltk
nltk.download('punkt')  # 确保 punkt 资源可用
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# 计算文本嵌入
def compute_text_embedding(text, max_sentences=10):
    sentences = sent_tokenize(text)
    selected = sentences[:max_sentences]
    weights = [3 if i < 5 else 1 for i in range(len(selected))]
    embeddings = model.encode(selected)
    weights = np.array(weights).reshape(-1, 1)
    return np.sum(embeddings * weights, axis=0) / np.sum(weights)

X_vectors = []
y_labels = []
titles = []

for label in categories:
    folder = os.path.join(data_dir, label)
    for filepath in glob.glob(os.path.join(folder, '*.txt')):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if not lines:
                    continue
                title = lines[0].strip()
                content = ' '.join([line.strip() for line in lines])
                vec = compute_text_embedding(content)
                X_vectors.append(vec)
                y_labels.append(label)
                titles.append(title)
        except Exception as e:
            print(f"❌ 读取 {filepath} 失败：{e}")

# 转换为 NumPy 数组
X_vectors = np.array(X_vectors)
y_labels = np.array(y_labels)
titles = np.array(titles)

print("✅ 成功加载样本数量：", len(X_vectors))
```
如果这一步 **成功加载样本**，那么你的 `train_test_split()` 就不会再报错了。

---

## **总结**
1. **先确保 NLTK punkt 资源可用**
   ```python
   !rm -rf /root/nltk_data
   import nltk
   nltk.download('punkt')
   ```
   **然后重启 Colab 运行时**。

2. **检查 `data_dir` 是否正确**
   ```python
   print(os.listdir("/content/drive/MyDrive/dataset"))
   ```

3. **测试 `txt` 文件是否可读**
   ```python
   for category in categories:
       folder = os.path.join(data_dir, category)
       print(f"📂 {category}: {os.listdir(folder)[:5]}")
   ```

4. **测试 `sent_tokenize()` 是否可用**
   ```python
   text = "This is a test sentence. Here is another one."
   print(sent_tokenize(text))
   ```

5. **如果 `Total samples loaded: 0`，运行完整的数据加载调试代码**

请逐步执行这些步骤，并告诉我哪一步仍然有问题，我会继续帮助你解决！🚀
