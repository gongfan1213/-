**(B) EVOLUTIONARY ALGORITHMS (30 MARKS)**

**8. Improving K-Means with Evolutionary Algorithms**  
K-Means clustering can be enhanced using evolutionary algorithms (EAs) by optimizing the cluster centroids using a population-based approach. A genetic algorithm (GA) can be employed where each individual in the population represents a set of k centroids. The steps include:
- Initializing a population of k centroid sets.
- Evaluating the fitness of each individual using an objective function such as minimizing intra-cluster variance.
- Applying selection, crossover, and mutation to generate new centroid sets.
- Iterating until convergence criteria are met.

This approach can prevent K-Means from getting stuck in local optima and improve clustering performance.

---

**9. Probability of No Mutation in Bit-Flip and Uniform Mutation**  
For a binary solution of length **n**, the probability that no bit is flipped:
- **Uniform Mutation:** Each bit has an independent probability **p** of flipping. The probability that no bit flips is:  
  \( P_{uniform} = (1 - p)^n \)
- **Single Bit-Flip Mutation:** Exactly one bit is flipped, so the probability that no mutation occurs is simply **0** because mutation is always applied.

---

**10. Reachability in NoMutAritGA**  
Since NoMutAritGA uses **arithmetic crossover without mutation**, only solutions that are convex combinations of current individuals can be generated. Given the population:
- \( x_1 = [1.5, 2.8, 3.3] \), \( x_2 = [1.5, 2.8, 3.3] \), \( x_3 = [0.2, 1.1, 2.2] \), \( x_4 = [0.3, 0.1, 2.2] \),
- Solutions **A = [0.5,2.5,1.1]** and **B = [1.1,0.5,2.5]** may not be reachable unless they are convex combinations of the existing individuals.

---

**11. Relationship Between MyGA(f, seed) and MyGA(g, seed)**  
Since both runs use the same seed, selection and crossover will operate on the same individuals. If mutation probability is low, early populations will be similar. Changing operators affects this correlation, making MyGA(f) and MyGA(g) diverge more quickly.

---

**12. Comparing DE and RS on Function h**  
The function h(x) is linear except at \( x = [0,0,0,0] \). DE is expected to outperform RS as it directs search towards optimal solutions instead of random sampling. DE benefits from gradient-like optimization, reducing evaluations needed to reach the global minimum.

---

**(C) ASSOCIATION RULES MINING (20 MARKS)**

**13. Apriori Algorithm Application**  
(a) **Candidate and Frequent Itemsets:**
- L1 (single items with support ≥ 33%)
  - {Guitar} (3/6), {DrumSet} (4/6), {Microphone} (2/6), {Amplifier} (2/6), {GuitarPick} (3/6)  
- L2 (pairs)
  - {Guitar, GuitarPick} (2/6), {DrumSet, Guitar} (2/6), {DrumSet, GuitarPick} (2/6)  
- L3 (triples)  
  - None meet minsup  

(b) **Final Frequent Itemsets:** \{Guitar\}, \{DrumSet\}, \{GuitarPick\}, \{Guitar, GuitarPick\}, \{DrumSet, Guitar\}, \{DrumSet, GuitarPick\}

(c) **Number of Database Scans:** 3 (one per itemset level)

(d) **Association Rules (support/confidence):**
- Guitar → GuitarPick (33%, 66.7%) ✅
- GuitarPick → Guitar (33%, 100%) ✅
- DrumSet → GuitarPick (33%, 50%) ❌
- DrumSet → Guitar (33%, 50%) ❌

(e) **Valid Rules Sorted by Lift:**
- GuitarPick → Guitar (Lift = 1.5)
- Guitar → GuitarPick (Lift = 1.0)

---

**14. Weighted Apriori Strategy**  
Modify itemset support computation by incorporating transaction weights (e.g., transaction cost). Keep Apriori’s core structure intact but redefine support as:
  \[ support(I) = \frac{\sum_{T \in D} weight(T) \cdot I(T)}{\sum_{T \in D} weight(T)} \]
This ensures higher-value transactions influence frequent itemsets more, without altering Apriori’s mechanics.

