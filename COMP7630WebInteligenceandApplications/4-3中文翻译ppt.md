文档从44页开始的内容翻译如下：
### 主成分分析
将特征分解应用于协方差矩阵$Cov(X)$（把$\frac{1}{m}$融入$X^TX$ ，并且$Cov(X)$是对称矩阵）。
\[X^TX = Qdiag(\lambda)Q^T\]
\[Q^TX^TXQ = Q^TQdiag(\lambda)Q^TQ\]
\[(XQ)^T(XQ) = diag(\lambda)\]
变换变为：$Y = XQ$
$Q$的列向量：主成分向量（特征向量） 。$diag(\lambda)$ ：在新坐标系中$Y$的方差（特征值）。
### 主成分分析
\[Y = XQ = 
\begin{bmatrix}
a^T \\ 
b^T \\ 
c^T \\ 
\vdots 
\end{bmatrix}
\begin{bmatrix}
q_1 \\ 
q_2 \\ 
\vdots
\end{bmatrix}\]
### 降维
仅保留前$k$个主成分向量
\[X^TX \approx Q_kdiag(\lambda_k)Q_k^T\]
变换变为：$Y = XQ_k \in \mathbb{R}^{m×k}$
$Q_k$的列向量：前$k$个主成分向量（特征向量）
$diag(\lambda_k)$ ：前$k$个特征值
### 主成分分析的实践
通常你会有一个数据矩阵$A$ ，具有以下特点：
 - 它的行是记录。
 - 它的列是（数值）特征，即记录的不同变量。
 - 所以任何一条记录都有多个特征（即它的列）。
我们能否减少对特定任务有用的特征数量呢？
 - 什么任务？
可视化（降维到二维）、学习（分类或回归）等。
 - 如何减少？
对每一列进行中心化（通过减去列均值），假设得到的矩阵是$X$ 。
计算$X^TX$ ，这是特征的协方差矩阵（显然是对称矩阵）。
计算特征分解$X^TX = Qdiag(\lambda)Q^T$ ，其中$diag(\lambda)$中的特征值按降序排列。
根据需求确定新的维度$k$ 。
选择$Q$的前$k$列并将其称为$Q_k$ 。
这样你也选择了$Q^T$的前$k$行以及$diag(\lambda)$左上角$k×k$的子矩阵。新的降维数据矩阵是$B = AQ_k$ 。
### 解释方差比
 - 原始数据矩阵$A$有$n$个特征/列。
 - 降维后的数据矩阵$B$有$k$个特征/列。$B$是通过选择总共$n$个特征值中最大的$k$个特征值得到的。
 - 每个特征值表示沿着特征基（相应的特征向量）$k$轴的方差。
 - （降维的）解释方差比由下式给出：
（公式中，外层求和的每一项是第$i$个主成分（或特征向量）的解释方差 ）
直观地说，解释方差比就是保留信息的百分比！
### 如何确定主成分分析中的成分数量？
一般来说，这是在尽可能保留信息和降低数据复杂度之间的权衡。
然而，这取决于你试图解决的具体问题。以下是一些可能的用例：
 - 如果目标是可视化（例如绘制记录的散点图），选择2或3个成分（否则无法绘制记录）。
 - 如果目标是压缩数据，因为后续的处理步骤（例如分类或聚类）计算成本较高，那么应该根据所需的容忍度选择成分数量。
 - 如果目标是提高后续处理步骤（例如分类或聚类）的有效性/准确性，并且计算时间不是问题，那么尝试不同的值并为潜在任务选择最佳值（如果是分类或回归，则使用著名的交叉验证方法）。
 - 两种在许多情况下都有效的常用启发式方法如下：
保留能够解释数据总方差很大一部分的成分（例如，解释90%、95%或99%的方差都是合理的选择）。
“肘部法则”：运行主成分分析，成分数量等于整个维度；绘制“碎石图”（一种简单的图，$x$轴是主成分，$y$轴是解释方差）；通过观察图，选择每个后续主成分解释的方差比例下降的点；使用选定的成分数量重新运行主成分分析。
### 数据矩阵的标准化
在实际情况中，预处理不仅限于减去均值。
通常，会对所有变量进行标准化。
 - 标准化：
数据矩阵的任何一列代表一个变量的值。
减去该列的均值。
除以该列的标准差。
此时该列的均值为0，标准差为1。
新的值称为$z$分数，它表示一个值相对于均值偏离了多少个标准差。
### Python中的主成分分析
在Python文件pca_example.py中可以看到在鸢尾花数据集上进行主成分分析的演示。
为了运行它，你需要按如下方式安装Python模块：pip install numpy sklearn matplotlib
### 主成分分析的可解释性
主成分非常有用，但可能缺乏可解释性。
虽然我们知道通过主成分分析获得的任何特征都是现有特征的线性组合。
线性组合中的系数称为“载荷”，其绝对值衡量了一个特征对某个主成分的重要程度。
如果你要求新特征更具可解释性，该怎么办？
使用特征选择方法！
它们选择特征的子集，从而减少了特征数量，但所选特征保留了其原始含义。
其解释方差比主成分分析要差。
使用哪种特征选择方法呢？
Python的Scikit Learn库实现了递归特征消除（sklearn.feature_selection.RFE）。
使用具有二进制表示的进化算法，并为手头的任务专门定义目标函数。 
