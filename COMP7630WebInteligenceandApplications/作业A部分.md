ä¸‹é¢ç»™å‡ºä¸€ä¸ªå®Œæ•´çš„ Google Colab Notebook ç¤ºä¾‹ä»£ç ï¼Œæ¶µç›–äº†ä½œä¸š A çš„æ‰€æœ‰è¦æ±‚ã€‚è¯·ç¡®ä¿ä½ å·²å°†æ•°æ®é›†è§£å‹åä¸Šä¼ åˆ°ä½ çš„äº‘ç«¯ç¡¬ç›˜ä¸­ï¼Œå¹¶ä¸”è·¯å¾„ä¸º  
```
/content/drive/MyDrive/dataset/dataset
```  
ï¼ˆå³ç‚¹å‡»äº‘ç«¯ç¡¬ç›˜ä¸­çš„â€œdatasetâ€ï¼Œå†ç‚¹å‡»â€œdatasetâ€ï¼Œè¿›å…¥åŒ…å«äº”ä¸ªå­æ–‡ä»¶å¤¹ï¼šcakes, chocolate_bars, cookies, soft_drinks, teasï¼‰ã€‚åŒæ—¶ï¼Œæœ¬ä»£ç ä½¿ç”¨äº† nltkã€sentence-transformersã€scikit-learnã€wikipedia ç­‰åº“ï¼Œå¦‚æœ‰éœ€è¦å¯å…ˆè¿è¡Œ pip install å‘½ä»¤å®‰è£…ç›¸å…³ä¾èµ–ã€‚

è¯·å°†ä¸‹é¢ä»£ç å¤åˆ¶åˆ° Colab Notebook ä¸­è¿è¡Œï¼Œä»£ç ä¸­å·²è¯¦ç»†æ³¨é‡Šäº†å„éƒ¨åˆ†å¯¹åº”é¢˜å·ï¼Œå¹¶ä¸”å­¦å·å·²æ›´æ–°ä¸º 24439169ã€‚

---

```python
# ================================
# Notebook for COMP7630 Assignment A:
# Text Vectorization, Classification & Clustering
# å­¦å·ï¼š24439169
# ================================

# ---- é¢„å…ˆå®‰è£…æ‰€éœ€åº“ï¼ˆå¦‚æœå·²å®‰è£…å¯æ³¨é‡Šæ‰ï¼‰ ----
!pip install -q sentence-transformers wikipedia nltk

# ---- æŒ‚è½½ Google Driveï¼ˆæ•°æ®é›†å­˜å‚¨åœ¨äº‘ç«¯ç¡¬ç›˜ï¼‰----
from google.colab import drive
drive.mount('/content/drive')

# ---- å¯¼å…¥å¸¸ç”¨åº“ ----
import os
import glob
import numpy as np
import matplotlib.pyplot as plt
import zipfile

# è®¾ç½®éšæœºç§å­ï¼ŒæŒ‰ç…§é¢˜ç›®è¦æ±‚
student_id = 24439169  # æ›¿æ¢ä¸ºä½ çš„å­¦å·
np.random.seed(student_id)

# ---- ä¸‹è½½ nltk æ•°æ®ï¼ˆå¥å­åˆ†å‰²ç”¨ï¼‰----
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

# ---- åŠ è½½ SentenceTransformer æ¨¡å‹ ----
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

# ================================
# Q1: Load Data and Compute Text Embeddings
# ================================

# æ•°æ®é›†è·¯å¾„ï¼šå‡è®¾è§£å‹ååœ¨äº‘ç›˜ä¸­è·¯å¾„ä¸º /content/drive/MyDrive/dataset/dataset
# æ•°æ®é›†ç›®å½•ç»“æ„ä¸ºï¼šcakes, chocolate_bars, cookies, soft_drinks, teas å„æ–‡ä»¶å¤¹ä¸­åŒ…å«è‹¥å¹² txt æ–‡ä»¶
data_dir = "/content/drive/MyDrive/dataset/dataset"

# å®šä¹‰å‡½æ•°ï¼šè®¡ç®—æ–‡æœ¬çš„åŠ æƒå¥å­åµŒå…¥
def compute_text_embedding(text, max_sentences=10):
    """
    å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œå¥å­åˆ†å‰²ï¼Œå–å‰10å¥ï¼ˆæˆ–ä¸è¶³10å¥åˆ™å…¨éƒ¨ï¼‰ã€‚
    å¯¹å‰5å¥èµ‹äºˆæƒé‡3ï¼Œå…¶ä½™å¥å­èµ‹äºˆæƒé‡1ï¼Œ
    è®¡ç®—åŠ æƒå¹³å‡åçš„å¥å­åµŒå…¥ã€‚
    """
    sentences = sent_tokenize(text)
    selected = sentences[:max_sentences]
    # æƒé‡è®¾ç½®ï¼šå‰5å¥æƒé‡ä¸º3ï¼Œå…¶ä½™æƒé‡ä¸º1
    weights = [3 if i < 5 else 1 for i in range(len(selected))]
    # è®¡ç®—å„å¥å­çš„åµŒå…¥
    embeddings = model.encode(selected)
    weights = np.array(weights).reshape(-1, 1)
    weighted_avg = np.sum(embeddings * weights, axis=0) / np.sum(weights)
    return weighted_avg

# åˆå§‹åŒ–å­˜å‚¨åˆ—è¡¨
X_vectors = []   # å­˜å‚¨æ–‡æœ¬å‘é‡
y_labels = []    # å­˜å‚¨ç±»åˆ«æ ‡ç­¾
titles = []      # å­˜å‚¨æ–‡æœ¬æ ‡é¢˜ï¼ˆæ–‡ä»¶ç¬¬ä¸€è¡Œï¼‰
file_paths = []  # å­˜å‚¨æ–‡ä»¶è·¯å¾„ï¼ˆå¯ç”¨äºåç»­è°ƒè¯•ï¼‰

# æ•°æ®é›†ç›®å½•ä¸­å„ç±»åˆ«åç§°ï¼ˆæ–‡ä»¶å¤¹åéœ€ä¸å®é™…ä¿æŒä¸€è‡´ï¼‰
categories = ['cakes', 'chocolate_bars', 'cookies', 'soft_drinks', 'teas']

# éå†æ¯ä¸ªç±»åˆ«ç›®å½•ï¼Œè¯»å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
for label in categories:
    folder = os.path.join(data_dir, label)
    for filepath in glob.glob(os.path.join(folder, '*.txt')):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if not lines:
                    continue
                # ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
                title = lines[0].strip()
                # å‰©ä½™å†…å®¹ä½œä¸ºæ–‡æœ¬æ­£æ–‡
                content = ' '.join([line.strip() for line in lines])
                vec = compute_text_embedding(content)
                X_vectors.append(vec)
                y_labels.append(label)
                titles.append(title)
                file_paths.append(filepath)
        except Exception as e:
            print(f"Error processing {filepath}: {e}")

# è½¬æ¢ä¸º numpy æ•°ç»„
X_vectors = np.array(X_vectors)
y_labels = np.array(y_labels)
titles = np.array(titles)

print("Total samples loaded:", len(X_vectors))

# è¿›è¡Œåˆ†å±‚åˆ’åˆ†ï¼š75% ä¸ºè®­ç»ƒé›† (X1, y1)ï¼Œ25% ä¸ºæµ‹è¯•é›† (X2, y2)
from sklearn.model_selection import train_test_split
X1, X2, y1, y2, titles_train, titles_test = train_test_split(
    X_vectors, y_labels, titles, test_size=0.25, stratify=y_labels, random_state=student_id
)

print("Training set shapes: X1 =", X1.shape, ", y1 =", y1.shape)
print("Test set shapes: X2 =", X2.shape, ", y2 =", y2.shape)

# ================================
# Q2: 2D Scatter Plot using PCA
# ================================
from sklearn.decomposition import PCA

# å¯¹æ•´ä¸ªæ•°æ®é›† X_vectors ä½¿ç”¨ PCA é™ç»´è‡³ 2D
pca_2d = PCA(n_components=2)
X_pca = pca_2d.fit_transform(X_vectors)
explained_variance = np.sum(pca_2d.explained_variance_ratio_) * 100

# ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œæ ¹æ®ç±»åˆ«ä¸Šè‰²
plt.figure(figsize=(8,6))
for label in np.unique(y_labels):
    idx = (y_labels == label)
    plt.scatter(X_pca[idx, 0], X_pca[idx, 1], label=label, alpha=0.7)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title(f'2D PCA Scatter Plot (Explained Variance: {explained_variance:.2f}%)')
plt.legend()
plt.show()

print("Total explained variance (2D PCA): {:.2f}%".format(explained_variance))

# ================================
# Q3: Classification with Cross-Validation
# ================================
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score

# å®šä¹‰ PCA é¢„å¤„ç†ï¼Œä¿ç•™ 95% æ–¹å·®
pca_95 = PCA(n_components=0.95)

# å®šä¹‰ä¸‰ç§åˆ†ç±»å™¨åŠå…¶å‚æ•°
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=student_id),
    'Random Forest': RandomForestClassifier(n_estimators=25, max_depth=5, random_state=student_id),
    'KNN': KNeighborsClassifier(n_neighbors=9)
}

# å®šä¹‰äº¤å‰éªŒè¯ç­–ç•¥ï¼šé‡å¤åˆ†å±‚ 5 æŠ˜äº¤å‰éªŒè¯ï¼Œé‡å¤ 5 æ¬¡
cv_strategy = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=student_id)

# é’ˆå¯¹æ¯ä¸ªåˆ†ç±»å™¨å»ºç«‹æµæ°´çº¿ï¼šæ ‡å‡†åŒ– -> PCA(95%) -> åˆ†ç±»å™¨
cv_results = {}
for name, clf in classifiers.items():
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('pca', PCA(n_components=0.95)),
        ('classifier', clf)
    ])
    scores = cross_val_score(pipeline, X1, y1, cv=cv_strategy, scoring='accuracy')
    cv_results[name] = scores
    print(f"{name}: Mean Accuracy = {np.mean(scores):.4f}, Std = {np.std(scores):.4f}")

# ================================
# Q4: Select Best Classifier and Evaluate on Test Set
# ================================
# é€‰æ‹©äº¤å‰éªŒè¯ä¸­å¹³å‡å‡†ç¡®ç‡æœ€é«˜çš„åˆ†ç±»å™¨
best_clf_name = max(cv_results, key=lambda k: np.mean(cv_results[k]))
print(f"\nBest classifier based on CV: {best_clf_name}")

# æ ¹æ®æœ€ä½³åˆ†ç±»å™¨æ„å»ºæµæ°´çº¿
if best_clf_name == 'Logistic Regression':
    best_clf = LogisticRegression(max_iter=1000, random_state=student_id)
elif best_clf_name == 'Random Forest':
    best_clf = RandomForestClassifier(n_estimators=25, max_depth=5, random_state=student_id)
elif best_clf_name == 'KNN':
    best_clf = KNeighborsClassifier(n_neighbors=9)

pipeline_best = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('classifier', best_clf)
])
pipeline_best.fit(X1, y1)
y_pred = pipeline_best.predict(X2)

# æ€»ä½“æµ‹è¯•å‡†ç¡®ç‡
from sklearn.metrics import accuracy_score, classification_report
overall_acc = accuracy_score(y2, y_pred)
print("Overall accuracy on test set: {:.4f}".format(overall_acc))

# æŒ‰ç±»åˆ«è®¡ç®—å‡†ç¡®ç‡å’Œåˆ—å‡ºè¯¯åˆ†ç±»æ–‡æœ¬ï¼ˆè¾“å‡ºæ ‡é¢˜å’ŒçœŸå®æ ‡ç­¾ï¼‰
import pandas as pd
results_df = pd.DataFrame({'Title': titles_test, 'True Label': y2, 'Predicted': y_pred})
print("\nPer-class performance and misclassified samples:")
for label in np.unique(y2):
    idx = (results_df['True Label'] == label)
    acc = accuracy_score(results_df.loc[idx, 'True Label'], results_df.loc[idx, 'Predicted'])
    print(f"\nClass '{label}': Accuracy = {acc:.4f}")
    misclassified = results_df[(results_df['True Label'] == label) & (results_df['Predicted'] != label)]
    if not misclassified.empty:
        print("Misclassified samples:")
        for _, row in misclassified.iterrows():
            print(f"  Title: {row['Title']}   True Label: {row['True Label']}")
    else:
        print("No misclassified samples.")

# ================================
# Q5: K-Means Clustering on the Entire Dataset
# ================================
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

# å¯¹æ•´ä¸ªæ•°æ®é›† X_vectors è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¹¶ä½¿ç”¨ PCA ä¿ç•™95%æ–¹å·®
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_vectors)
pca_95_full = PCA(n_components=0.95)
X_pca_full = pca_95_full.fit_transform(X_scaled)

cluster_results = {}
for k in [4, 5, 6]:
    kmeans = KMeans(n_clusters=k, random_state=student_id)
    cluster_labels = kmeans.fit_predict(X_pca_full)
    inertia = kmeans.inertia_
    sil_score = silhouette_score(X_pca_full, cluster_labels)
    cluster_results[k] = {'inertia': inertia, 'silhouette': sil_score, 'labels': cluster_labels, 'model': kmeans}
    print(f"\nk = {k}: Inertia = {inertia:.2f}, Silhouette Score = {sil_score:.4f}")

# ================================
# Q6: Select Best Clustering and Generate LLM Prompt for Naming Clusters
# ================================
# é€‰æ‹© silhouette åˆ†æ•°æœ€é«˜çš„èšç±»ç»“æœä½œä¸ºæœ€ä½³èšç±»
best_k = max(cluster_results, key=lambda k: cluster_results[k]['silhouette'])
print(f"\nBest clustering selected: k = {best_k} (Silhouette Score = {cluster_results[best_k]['silhouette']:.4f})")

# æ ¹æ®æ¯ä¸ªç°‡ä¸­éƒ¨åˆ†æ ·æœ¬æ ‡é¢˜ç”Ÿæˆä¸€ä¸ª LLM æç¤ºï¼Œè¦æ±‚ LLM ç»™æ¯ä¸ªç°‡å‘½åå¹¶è¯´æ˜ç†ç”±
best_labels = cluster_results[best_k]['labels']
prompt_lines = []
prompt_lines.append(f"Please provide descriptive names for the following {best_k} clusters. The clusters are formed based on blog article titles. Below are sample titles from each cluster:")

for cluster_id in range(best_k):
    sample_titles = titles[best_labels == cluster_id][:5]  # æ¯ä¸ªç°‡æœ€å¤šå–5ä¸ªæ ·æœ¬æ ‡é¢˜
    prompt_lines.append(f"\nCluster {cluster_id}:")
    for t in sample_titles:
        prompt_lines.append(f"- {t}")
prompt_lines.append("\nPlease suggest a descriptive name for each cluster and briefly explain your reasoning.")
llm_prompt = "\n".join(prompt_lines)
print("\nGenerated LLM prompt for cluster naming:\n")
print(llm_prompt)

# ï¼ˆæ³¨æ„ï¼šè¯·å°†ä¸Šè¿°æç¤ºå¤åˆ¶åˆ°å…¬å¼€çš„ LLMï¼ˆä¾‹å¦‚ ChatGPTï¼‰ä¸­ï¼Œå¹¶å°†ç”Ÿæˆçš„å‘½åæˆªå›¾ç²˜è´´åˆ° Notebook ä¸­ã€‚ï¼‰

# ================================
# Q7: Scrape "Egg tart" Wikipedia Page and Recommend Related Articles
# ================================
import wikipedia
from sklearn.metrics.pairwise import cosine_similarity

# çˆ¬å– Wikipedia ä¸­ "Egg tart" é¡µé¢å†…å®¹
try:
    egg_tart_page = wikipedia.page("Egg tart")
    egg_tart_content = egg_tart_page.content
    egg_tart_title = egg_tart_page.title
    print(f"\nSuccessfully scraped Wikipedia page: {egg_tart_title}")
except Exception as e:
    print("Failed to scrape 'Egg tart' page:", e)
    egg_tart_content = ""

# è®¡ç®— "Egg tart" é¡µé¢æ–‡æœ¬çš„åµŒå…¥
egg_tart_embedding = model.encode(egg_tart_content)

# è®¡ç®— "Egg tart" ä¸æ•´ä¸ªæ•°æ®é›†ä¸­å„ç¯‡æ–‡ç« ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
similarities = cosine_similarity([egg_tart_embedding], X_vectors)[0]

# é€‰å–ç›¸ä¼¼åº¦æœ€é«˜çš„ 5 ç¯‡æ–‡ç« ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åºï¼‰
top5_idx = np.argsort(similarities)[-5:][::-1]
print("\nRecommended 5 articles for 'Egg tart' (sorted by relevance):")
for idx in top5_idx:
    print(f"Title: {titles[idx]}, Category: {y_labels[idx]}, Similarity: {similarities[idx]:.4f}")

# æ–¹æ³•è¯´æ˜ï¼š
# 1. çˆ¬å– "Egg tart" é¡µé¢å¹¶è®¡ç®—å…¶æ–‡æœ¬åµŒå…¥ã€‚
# 2. å¯¹æ¯”æ•°æ®é›†ä¸­æ¯ç¯‡æ–‡ç« çš„åµŒå…¥ï¼ˆä½¿ç”¨ç›¸åŒçš„ SentenceTransformer æ¨¡å‹ï¼‰ã€‚
# 3. æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æ’åºï¼Œé€‰å–å‰ 5 ç¯‡æ–‡ç« ä½œä¸ºæ¨èï¼Œè®¤ä¸ºç›¸ä¼¼åº¦è¶Šé«˜æ–‡ç« å†…å®¹ç›¸å…³æ€§è¶Šå¼ºã€‚
```

---

### ä»£ç è¯´æ˜

1. **æ•°æ®åŠ è½½ä¸å‘é‡åŒ– (Q1)ï¼š**  
   - æŒ‚è½½ Google Drive åï¼Œä»æŒ‡å®šç›®å½•ä¸­éå†æ¯ä¸ªç±»åˆ«æ–‡ä»¶å¤¹ä¸­çš„ txt æ–‡ä»¶ã€‚  
   - ä½¿ç”¨ nltk çš„å¥å­åˆ†å‰²ï¼Œå¯¹å‰ 10 å¥è¿›è¡ŒåŠ æƒï¼ˆå‰ 5 å¥æƒé‡ 3ï¼Œå…¶ä½™æƒé‡ 1ï¼‰è®¡ç®—åŠ æƒå¹³å‡çš„å¥å­åµŒå…¥ã€‚  
   - åŒæ—¶ä¿å­˜æ–‡æœ¬çš„æ ‡é¢˜ï¼ˆæ–‡ä»¶ç¬¬ä¸€è¡Œï¼‰ï¼Œä¾¿äºåç»­è¾“å‡ºè¯¯åˆ†ç±»ç»“æœæ—¶æ˜¾ç¤ºã€‚

2. **æ•°æ®é›†åˆ’åˆ†ï¼š**  
   - ä½¿ç”¨ stratified split å°†æ•°æ®é›†åˆ’åˆ†ä¸º 75% çš„è®­ç»ƒé›†å’Œ 25% çš„æµ‹è¯•é›†ï¼Œå¹¶æ‰“å°æ•°æ®å½¢çŠ¶ã€‚

3. **PCA å¯è§†åŒ– (Q2)ï¼š**  
   - å¯¹æ•´ä¸ªæ•°æ®é›†é™è‡³äºŒç»´ï¼Œå¹¶ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œå„ç±»åˆ«ç”¨ä¸åŒé¢œè‰²è¡¨ç¤ºï¼ŒåŒæ—¶æ‰“å° PCA çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹ã€‚

4. **åˆ†ç±»å™¨è®­ç»ƒä¸äº¤å‰éªŒè¯ (Q3)ï¼š**  
   - å»ºç«‹ä¸‰ä¸ªåˆ†ç±»å™¨çš„æµæ°´çº¿ï¼šæ ‡å‡†åŒ– -> PCA (ä¿ç•™95%æ–¹å·®) -> åˆ†ç±»å™¨ã€‚  
   - ä½¿ç”¨é‡å¤åˆ†å±‚ k æŠ˜äº¤å‰éªŒè¯ï¼ˆ5 æŠ˜ï¼Œé‡å¤ 5 æ¬¡ï¼‰è¯„ä¼°å„æ¨¡å‹ï¼Œè¾“å‡ºå¹³å‡å‡†ç¡®ç‡å’Œæ ‡å‡†å·®ã€‚

5. **æœ€ä½³åˆ†ç±»å™¨çš„é€‰æ‹©ä¸æµ‹è¯•é›†è¯„ä¼° (Q4)ï¼š**  
   - æ ¹æ®äº¤å‰éªŒè¯ç»“æœé€‰æ‹©è¡¨ç°æœ€å¥½çš„åˆ†ç±»å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒååœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼Œå¹¶è¾“å‡ºæ€»ä½“å‡†ç¡®ç‡ã€å„ç±»åˆ«å‡†ç¡®ç‡ä»¥åŠè¯¯åˆ†ç±»æ–‡æœ¬çš„æ ‡é¢˜å’ŒçœŸå®æ ‡ç­¾ã€‚

6. **K-Means èšç±» (Q5)ï¼š**  
   - å¯¹æ•´ä¸ªæ•°æ®é›†å…ˆè¿›è¡Œæ ‡å‡†åŒ–å’Œ PCAï¼ˆä¿ç•™95%æ–¹å·®ï¼‰ï¼Œç„¶ååˆ†åˆ«å¯¹ k = 4, 5, 6 è¿›è¡Œ K-Means èšç±»ï¼Œè¾“å‡ºæ¯ä¸ªèšç±»ç»“æœçš„ inertia å’Œ silhouette åˆ†æ•°ã€‚

7. **ç”Ÿæˆ LLM èšç±»å‘½åæç¤º (Q6)ï¼š**  
   - æ ¹æ® silhouette åˆ†æ•°é€‰æ‹©æœ€ä½³èšç±»ç»“æœï¼Œé’ˆå¯¹æ¯ä¸ªç°‡æŠ½å–éƒ¨åˆ†æ ·æœ¬æ ‡é¢˜ç”Ÿæˆä¸€ä¸ª LLM æç¤ºï¼Œè¦æ±‚ä¸ºå„èšç±»å‘½åå¹¶è¯´æ˜ç†ç”±ã€‚

8. **çˆ¬å– Wikipedia â€œEgg tartâ€ é¡µé¢ä¸æ–‡ç« æ¨è (Q7)ï¼š**  
   - ä½¿ç”¨ wikipedia æ¨¡å—çˆ¬å– â€œEgg tartâ€ é¡µé¢ï¼Œè®¡ç®—å…¶åµŒå…¥å¹¶ä¸æ•°æ®é›†ä¸­æ‰€æœ‰æ–‡ç« çš„åµŒå…¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé€‰å–ç›¸ä¼¼åº¦æœ€é«˜çš„ 5 ç¯‡æ–‡ç« æ¨èï¼Œå¹¶æŒ‰ç›¸ä¼¼åº¦é™åºè¾“å‡ºã€‚

å°†æ­¤ Notebook ä¸Šä¼ è‡³ Google Colab åè¿è¡Œï¼Œå³å¯å®Œæˆä½œä¸š A çš„æ‰€æœ‰å†…å®¹ã€‚è‹¥æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´ï¼Œå¯ç»§ç»­æ²Ÿé€šã€‚
ä½ çš„é—®é¢˜ä»ç„¶æ˜¯ **NLTK punkt èµ„æºä¸¢å¤±** å’Œ **æ•°æ®é›†æ²¡æœ‰æ­£ç¡®åŠ è½½** å¯¼è‡´ `train_test_split()` å¤±è´¥ã€‚è®©æˆ‘ä»¬é€æ­¥è§£å†³é—®é¢˜ã€‚

---

## **ç¬¬ä¸€æ­¥ï¼šä¿®å¤ NLTK punkt èµ„æºä¸¢å¤±**
Colab å¯èƒ½æœ‰ç¼“å­˜é—®é¢˜ï¼Œå› æ­¤ä½ éœ€è¦å¼ºåˆ¶åˆ é™¤ `nltk_data` ç›®å½•å¹¶é‡æ–°ä¸‹è½½ `punkt` èµ„æºï¼š

```python
!rm -rf /root/nltk_data  # åˆ é™¤ NLTK å¯èƒ½æŸåçš„ç¼“å­˜
import nltk
nltk.download('punkt')  # é‡æ–°ä¸‹è½½ punkt èµ„æº
```
ç„¶å**é‡å¯ Colab è¿è¡Œæ—¶**ï¼ˆç‚¹å‡» `Runtime` -> `Restart runtime`ï¼‰ã€‚

---

## **ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®åŠ è½½**
ä½ çš„é”™è¯¯ä¿¡æ¯ `Total samples loaded: 0` è¡¨æ˜ **æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½• txt æ–‡ä»¶**ã€‚è¯·è¿è¡Œä»¥ä¸‹ä»£ç æ£€æŸ¥æ•°æ®é›†è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼š

```python
import os

# ä½ çš„æ•°æ®é›†è·¯å¾„
data_dir = "/content/drive/MyDrive/dataset/dataset"

# ç¡®ä¿æ•°æ®é›†ç›®å½•å­˜åœ¨
if os.path.exists(data_dir):
    print("âœ… æ•°æ®é›†ç›®å½•å­˜åœ¨")
else:
    print("âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")

# åˆ—å‡º dataset ç›®å½•ä¸­çš„æ–‡ä»¶å¤¹
print("ğŸ“‚ dataset ç›®å½•å†…å®¹ï¼š", os.listdir(data_dir))

# æ£€æŸ¥å­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
categories = ['cakes', 'chocolate_bars', 'cookies', 'soft_drinks', 'teas']
for category in categories:
    category_path = os.path.join(data_dir, category)
    if os.path.exists(category_path):
        print(f"âœ… å‘ç°ç±»åˆ«æ–‡ä»¶å¤¹ï¼š{category}")
        print("ğŸ“œ æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰5ä¸ªï¼‰ï¼š", os.listdir(category_path)[:5])
    else:
        print(f"âŒ æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹ï¼š{category}")
```
#### **å¯èƒ½çš„æƒ…å†µ**
1. **å¦‚æœ `âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨`ï¼Œè¯´æ˜ `data_dir` è·¯å¾„é”™è¯¯**  
   - **è§£å†³æ–¹æ¡ˆï¼š** æ‰‹åŠ¨æ£€æŸ¥ Google Drive çš„å®é™…è·¯å¾„ï¼Œå¹¶æ›´æ–° `data_dir`ã€‚  
   - ä½ å¯ä»¥åœ¨ Colab è¿è¡Œ `!ls /content/drive/MyDrive/` æ‰¾åˆ°æ­£ç¡®çš„è·¯å¾„ã€‚

2. **å¦‚æœ `âŒ æœªæ‰¾åˆ°ç±»åˆ«æ–‡ä»¶å¤¹`ï¼Œè¯´æ˜ `dataset` ç›®å½•ç»“æ„é”™è¯¯**  
   - å¯èƒ½ä½  **è§£å‹åå°‘äº†ä¸€å±‚ç›®å½•**ï¼Œåº”è¯¥è¿›å…¥ `dataset/dataset/` è€Œä¸æ˜¯ `dataset/`ã€‚

3. **å¦‚æœ `âœ… æ•°æ®é›†ç›®å½•å­˜åœ¨` å¹¶åˆ—å‡ºäº† `txt` æ–‡ä»¶ï¼Œä½† `Total samples loaded: 0`**  
   - å¯èƒ½æ˜¯ `glob.glob(os.path.join(folder, '*.txt'))` å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨åˆ—å‡º `txt` æ–‡ä»¶ï¼š
   ```python
   for category in categories:
       folder = os.path.join(data_dir, category)
       txt_files = glob.glob(os.path.join(folder, '*.txt'))
       if txt_files:
           print(f"âœ… {category} ç›®å½•ä¸­æ‰¾åˆ° {len(txt_files)} ä¸ª txt æ–‡ä»¶")
       else:
           print(f"âŒ {category} ç›®å½•ä¸­æ²¡æœ‰ txt æ–‡ä»¶")
   ```

---

## **ç¬¬ä¸‰æ­¥ï¼šè°ƒè¯•æ•°æ®åŠ è½½**
å¦‚æœ **æ•°æ®ç›®å½•æ­£ç¡®ä½† `Total samples loaded: 0`**ï¼Œä½ éœ€è¦ **å•ç‹¬æµ‹è¯• `txt` æ–‡ä»¶æ˜¯å¦èƒ½æ­£ç¡®è¯»å–**ï¼š

```python
import glob

# é€‰å–æŸä¸ªç±»åˆ«çš„æ–‡ä»¶å¤¹
test_category = "teas"  # ä½ å¯ä»¥æ¢æˆ "cakes", "cookies" ç­‰
test_folder = os.path.join(data_dir, test_category)

# è·å–æ–‡ä»¶åˆ—è¡¨
txt_files = glob.glob(os.path.join(test_folder, '*.txt'))

# è¯»å–å‰2ä¸ªæ–‡ä»¶çš„å†…å®¹
for txt_file in txt_files[:2]:
    try:
        with open(txt_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"\nğŸ“œ è¯»å–æ–‡ä»¶: {txt_file}")
            print("ğŸ”¹ ç¬¬ä¸€è¡Œï¼ˆæ ‡é¢˜ï¼‰ï¼š", lines[0].strip() if lines else "ç©ºæ–‡ä»¶")
            print("ğŸ”¹ æ€»è¡Œæ•°ï¼š", len(lines))
    except Exception as e:
        print(f"âŒ è¯»å– {txt_file} å¤±è´¥ï¼š{e}")
```
#### **å¯èƒ½çš„æƒ…å†µ**
1. **å¦‚æœ `txt_files=[]`ï¼Œè¯´æ˜ `glob.glob()` æ‰¾ä¸åˆ° `txt` æ–‡ä»¶**  
   - å¯èƒ½ `*.txt` åŒ¹é…æ¨¡å¼æœ‰è¯¯ï¼Œå°è¯• `print(os.listdir(test_folder))` æ£€æŸ¥å®é™…æ–‡ä»¶åã€‚

2. **å¦‚æœ `è¯»å–æ–‡ä»¶å¤±è´¥`ï¼Œè¯´æ˜æŸäº› `txt` æ–‡ä»¶ç¼–ç æœ‰é—®é¢˜**  
   - å¯èƒ½ `utf-8` è§£æå¤±è´¥ï¼Œå¯ä»¥å°è¯• `encoding="ISO-8859-1"` æˆ– `encoding="utf-16"`ã€‚

---

## **ç¬¬å››æ­¥ï¼šç¡®è®¤æ•°æ®é›†æ­£ç¡®åï¼Œé‡æ–°è¿è¡Œä»£ç **
å¦‚æœä¸Šè¿°æ£€æŸ¥ **æ‰€æœ‰éƒ½æ­£å¸¸**ï¼Œä½†ä»ç„¶ `Total samples loaded: 0`ï¼Œä½ å¯ä»¥è¿è¡Œå®Œæ•´çš„æ•°æ®åŠ è½½ä»£ç ï¼š

```python
import nltk
nltk.download('punkt')  # ç¡®ä¿ punkt èµ„æºå¯ç”¨
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# è®¡ç®—æ–‡æœ¬åµŒå…¥
def compute_text_embedding(text, max_sentences=10):
    sentences = sent_tokenize(text)
    selected = sentences[:max_sentences]
    weights = [3 if i < 5 else 1 for i in range(len(selected))]
    embeddings = model.encode(selected)
    weights = np.array(weights).reshape(-1, 1)
    return np.sum(embeddings * weights, axis=0) / np.sum(weights)

X_vectors = []
y_labels = []
titles = []

for label in categories:
    folder = os.path.join(data_dir, label)
    for filepath in glob.glob(os.path.join(folder, '*.txt')):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if not lines:
                    continue
                title = lines[0].strip()
                content = ' '.join([line.strip() for line in lines])
                vec = compute_text_embedding(content)
                X_vectors.append(vec)
                y_labels.append(label)
                titles.append(title)
        except Exception as e:
            print(f"âŒ è¯»å– {filepath} å¤±è´¥ï¼š{e}")

# è½¬æ¢ä¸º NumPy æ•°ç»„
X_vectors = np.array(X_vectors)
y_labels = np.array(y_labels)
titles = np.array(titles)

print("âœ… æˆåŠŸåŠ è½½æ ·æœ¬æ•°é‡ï¼š", len(X_vectors))
```
å¦‚æœè¿™ä¸€æ­¥ **æˆåŠŸåŠ è½½æ ·æœ¬**ï¼Œé‚£ä¹ˆä½ çš„ `train_test_split()` å°±ä¸ä¼šå†æŠ¥é”™äº†ã€‚

---

## **æ€»ç»“**
1. **å…ˆç¡®ä¿ NLTK punkt èµ„æºå¯ç”¨**
   ```python
   !rm -rf /root/nltk_data
   import nltk
   nltk.download('punkt')
   ```
   **ç„¶åé‡å¯ Colab è¿è¡Œæ—¶**ã€‚

2. **æ£€æŸ¥ `data_dir` æ˜¯å¦æ­£ç¡®**
   ```python
   print(os.listdir("/content/drive/MyDrive/dataset"))
   ```

3. **æµ‹è¯• `txt` æ–‡ä»¶æ˜¯å¦å¯è¯»**
   ```python
   for category in categories:
       folder = os.path.join(data_dir, category)
       print(f"ğŸ“‚ {category}: {os.listdir(folder)[:5]}")
   ```

4. **æµ‹è¯• `sent_tokenize()` æ˜¯å¦å¯ç”¨**
   ```python
   text = "This is a test sentence. Here is another one."
   print(sent_tokenize(text))
   ```

5. **å¦‚æœ `Total samples loaded: 0`ï¼Œè¿è¡Œå®Œæ•´çš„æ•°æ®åŠ è½½è°ƒè¯•ä»£ç **

è¯·é€æ­¥æ‰§è¡Œè¿™äº›æ­¥éª¤ï¼Œå¹¶å‘Šè¯‰æˆ‘å“ªä¸€æ­¥ä»ç„¶æœ‰é—®é¢˜ï¼Œæˆ‘ä¼šç»§ç»­å¸®åŠ©ä½ è§£å†³ï¼ğŸš€

å¦‚æœ `sent_tokenize()` æ— æ³•è¿è¡Œï¼Œè¯´æ˜ `nltk` çš„ `punkt` èµ„æºä»ç„¶æœ‰é—®é¢˜ã€‚è¯·æŒ‰ç…§ä¸‹é¢æ­¥éª¤æ¥ä¿®å¤ï¼š

---

## **ç¬¬ä¸€æ­¥ï¼šå¼ºåˆ¶åˆ é™¤å¹¶é‡æ–°ä¸‹è½½ NLTK punkt èµ„æº**
è¯·åœ¨ Colab è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```python
!rm -rf /root/nltk_data  # åˆ é™¤å¯èƒ½æŸåçš„ NLTK èµ„æº
import nltk
nltk.download('punkt')  # é‡æ–°ä¸‹è½½ punkt
```
ç„¶å **é‡å¯ Colab è¿è¡Œæ—¶**ï¼ˆç‚¹å‡» `Runtime` -> `Restart runtime`ï¼‰ï¼Œå†è¿è¡Œï¼š

```python
from nltk.tokenize import sent_tokenize

# æµ‹è¯• sent_tokenize æ˜¯å¦å¯ç”¨
text = "This is a test sentence. Here is another one."
print(sent_tokenize(text))
```
å¦‚æœ `sent_tokenize(text)` ä»ç„¶æŠ¥é”™ï¼Œè¯·ç»§ç»­ä¸‹ä¸€æ­¥ã€‚

---

## **ç¬¬äºŒæ­¥ï¼šæ‰‹åŠ¨ä¸‹è½½ NLTK punkt èµ„æº**
å¦‚æœ `nltk.download('punkt')` æ— æ³•æ­£å¸¸å·¥ä½œï¼Œä½ å¯ä»¥ **æ‰‹åŠ¨ä¸‹è½½ punkt èµ„æº**ï¼š

```python
import nltk
nltk.download('punkt', download_dir='/usr/local/nltk_data')  # æŒ‡å®šä¸‹è½½ç›®å½•
```
ç„¶åè¿è¡Œï¼š
```python
import os
os.environ['NLTK_DATA'] = "/usr/local/nltk_data"

from nltk.tokenize import sent_tokenize
text = "This is a test sentence. Here is another one."
print(sent_tokenize(text))  # æœŸå¾…è¾“å‡ºä¸¤ä¸ªå¥å­
```

---

## **ç¬¬ä¸‰æ­¥ï¼šå¦‚æœä»ç„¶ä¸è¡Œï¼Œæ‰‹åŠ¨å®‰è£… NLTK**
å¦‚æœ `nltk.download()` ä»ç„¶å¤±è´¥ï¼Œå¯ä»¥å°è¯• **æ‰‹åŠ¨å®‰è£… NLTK**ï¼š
```python
!pip uninstall -y nltk
!pip install -U nltk
```
ç„¶å **é‡å¯ Colab è¿è¡Œæ—¶**ï¼Œé‡æ–°è¿è¡Œï¼š
```python
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

text = "This is a test sentence. Here is another one."
print(sent_tokenize(text))
```

---

### **æ€»ç»“**
1. **å…ˆåˆ é™¤æ—§çš„ NLTK èµ„æºå¹¶é‡æ–°ä¸‹è½½ `punkt`**
   ```python
   !rm -rf /root/nltk_data
   import nltk
   nltk.download('punkt')
   ```
   **ç„¶åé‡å¯ Colab è¿è¡Œæ—¶**ã€‚

2. **æ‰‹åŠ¨è®¾ç½® NLTK æ•°æ®è·¯å¾„**
   ```python
   import os
   os.environ['NLTK_DATA'] = "/usr/local/nltk_data"
   ```

3. **æµ‹è¯• `sent_tokenize()` æ˜¯å¦å¯ç”¨**
   ```python
   from nltk.tokenize import sent_tokenize
   print(sent_tokenize("This is a test sentence. Here is another one."))
   ```

4. **å¦‚æœä»ç„¶ä¸è¡Œï¼Œå°è¯•å¸è½½å¹¶é‡æ–°å®‰è£… NLTK**
   ```python
   !pip uninstall -y nltk
   !pip install -U nltk
   ```

è¯·é€æ­¥å°è¯•è¿™äº›æ–¹æ³•ï¼Œå¹¶å‘Šè¯‰æˆ‘å“ªä¸€æ­¥ä»ç„¶æœ‰é—®é¢˜ï¼Œæˆ‘ä¼šç»§ç»­å¸®åŠ©ä½ è§£å†³ï¼ğŸš€
æ ¹æ®ä»£ç å’Œè¾“å‡ºç»“æœçš„è¯¦ç»†åˆ†æï¼Œä½œä¸šAéƒ¨åˆ†çš„å®ç°åŸºæœ¬æ­£ç¡®ï¼Œå„é¢˜ç›®è¦æ±‚å‡å¾—åˆ°æ»¡è¶³ã€‚ä»¥ä¸‹æ˜¯å…·ä½“æ£€æŸ¥ç»“æœåŠå»ºè®®ï¼š

---

### **æ£€æŸ¥ç»“æœæ€»ç»“**

1. **Q1: æ•°æ®åŠ è½½ä¸æ–‡æœ¬å‘é‡åŒ–**  
   - **æ­£ç¡®æ€§**ï¼šæˆåŠŸåŠ è½½300ä¸ªæ ·æœ¬ï¼Œåˆ†å±‚åˆ’åˆ†è®­ç»ƒé›†ï¼ˆ225ï¼‰å’Œæµ‹è¯•é›†ï¼ˆ75ï¼‰ï¼Œæƒé‡åˆ†é…ç¬¦åˆé¢˜ç›®è¦æ±‚ã€‚  
   - **æ½œåœ¨æ”¹è¿›**ï¼šè‹¥æ–‡æœ¬å¥å­ä¸è¶³5å¥ï¼Œå‰æ‰€æœ‰å¥å­æƒé‡ä¸º3ï¼Œç¬¦åˆé¢˜æ„ï¼Œæ— éœ€ä¿®æ”¹ã€‚

2. **Q2: PCAé™ç»´ä¸å¯è§†åŒ–**  
   - **æ­£ç¡®æ€§**ï¼šæ­£ç¡®è®¡ç®—2D PCAå¹¶ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œç´¯è®¡è§£é‡Šæ–¹å·®23.36%ï¼ˆåˆç†ï¼‰ã€‚  
   - **æ³¨æ„**ï¼šè§£é‡Šæ–¹å·®è¾ƒä½æ˜¯æ–‡æœ¬é«˜ç»´æ•°æ®çš„å¸¸è§ç°è±¡ï¼Œæ— éœ€è°ƒæ•´ã€‚

3. **Q3: åˆ†ç±»æ¨¡å‹äº¤å‰éªŒè¯**  
   - **æ­£ç¡®æ€§**ï¼šæµæ°´çº¿è®¾è®¡åˆç†ï¼ˆæ ‡å‡†åŒ–â†’PCAâ†’åˆ†ç±»å™¨ï¼‰ï¼Œäº¤å‰éªŒè¯ç­–ç•¥æ­£ç¡®ã€‚  
   - **ç»“æœåˆ†æ**ï¼šKNNè¡¨ç°æœ€ä½³ï¼ˆ92.62%ï¼‰ï¼Œå¯èƒ½å› æ•°æ®å±€éƒ¨ç»“æ„æ˜æ˜¾ï¼Œç¬¦åˆé¢„æœŸã€‚

4. **Q4: æµ‹è¯•é›†è¯„ä¼°ä¸è¯¯åˆ†ç±»åˆ†æ**  
   - **æ­£ç¡®æ€§**ï¼šKNNæµ‹è¯•é›†å‡†ç¡®ç‡92%ï¼Œå„ç±»åˆ«å‡†ç¡®ç‡ä¸è¯¯åˆ†ç±»æ ‡é¢˜è¾“å‡ºå®Œæ•´ã€‚  
   - **æ³¨æ„**ï¼š'cookies'ç±»åˆ«å‡†ç¡®ç‡è¾ƒä½ï¼ˆ80%ï¼‰ï¼Œå¯èƒ½éœ€æ£€æŸ¥ç‰¹å¾åŒºåˆ†åº¦ã€‚

5. **Q5: K-Meansèšç±»**  
   - **æ­£ç¡®æ€§**ï¼šæ ‡å‡†åŒ–+PCAåèšç±»ï¼Œè®¡ç®—æƒ¯æ€§åŠè½®å»“ç³»æ•°ï¼Œk=5æ—¶æœ€ä¼˜ï¼ˆè½®å»“0.1011ï¼‰ã€‚  
   - **å¤‡æ³¨**ï¼šè½®å»“ç³»æ•°æ™®éåä½ï¼Œä½†ä»£ç æ— è¯¯ï¼Œå¯èƒ½æ•°æ®æœ¬èº«èšç±»éš¾åº¦é«˜ã€‚

6. **Q6: LLMæç¤ºç”Ÿæˆ**  
   - **æ­£ç¡®æ€§**ï¼šä»æ¯ä¸ªç°‡æŠ½å–5ä¸ªæ ‡é¢˜ç”Ÿæˆæç¤ºï¼Œé€»è¾‘æ¸…æ™°ï¼Œå¯ç›´æ¥ç”¨äºLLMã€‚

7. **Q7: ç›¸å…³æ–‡ç« æ¨è**  
   - **æ­£ç¡®æ€§**ï¼šåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„æ¨èæ–¹æ³•åˆç†ï¼Œè¾“å‡ºå‰5ç¯‡ç›¸å…³æ–‡ç« ã€‚  
   - **æ³¨æ„**ï¼šæ¨èç»“æœå«è·¨ç±»åˆ«æ–‡ç« ï¼ˆå¦‚â€œFortune cookieâ€ï¼‰ï¼Œä½†é¢˜ç›®æœªé™åˆ¶åŒç±»ï¼Œæ•…æ— è¯¯ã€‚

---

### **æ”¹è¿›å»ºè®®**

1. **Q3åˆ†ç±»æ¨¡å‹ä¼˜åŒ–**  
   - å¯å°è¯•è°ƒæ•´PCAä¿ç•™çš„æ–¹å·®æ¯”ä¾‹ï¼ˆå¦‚90%æˆ–æ›´é«˜ï¼‰ï¼Œè§‚å¯Ÿæ˜¯å¦å½±å“KNNè¡¨ç°ã€‚

2. **Q5èšç±»ç»“æœåˆ†æ**  
   - æ·»åŠ å¯è§†åŒ–ï¼ˆå¦‚èšç±»åçš„PCAæ•£ç‚¹å›¾ï¼‰ä»¥ç›´è§‚è¯„ä¼°ç°‡åˆ†å¸ƒã€‚

3. **Q7æ¨èç³»ç»Ÿæ‰©å±•**  
   - è‹¥éœ€æ¨èåŒç±»æ–‡ç« ï¼Œå¯åœ¨ç›¸ä¼¼åº¦è®¡ç®—åæŒ‰ç±»åˆ«è¿‡æ»¤ï¼Œä½†é¢˜ç›®æœªè¦æ±‚æ­¤æ­¥éª¤ã€‚

---

### **ç»“è®º**

ä»£ç å®ç°ç¬¦åˆé¢˜ç›®è¦æ±‚ï¼Œé€»è¾‘ä¸¥è°¨ï¼Œè¾“å‡ºç»“æœåˆç†ã€‚æ— é‡å¤§é”™è¯¯ï¼Œå¯é€šè¿‡è¿›ä¸€æ­¥ä¼˜åŒ–æå‡æ¨¡å‹è¡¨ç°ï¼Œä½†éå¿…éœ€ã€‚å»ºè®®ç”¨æˆ·æŒ‰éœ€è¡¥å……å¯è§†åŒ–æˆ–æ³¨é‡Šä»¥å¢å¼ºä»£ç å¯è¯»æ€§ã€‚

**æœ€ç»ˆæ£€æŸ¥ç»“æœï¼šæ­£ç¡® âœ…**
