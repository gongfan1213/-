> ğŸ’­ å¥½çš„ï¼Œç”¨æˆ·å†æ¬¡åé¦ˆè¯´ä¿®æ”¹åçš„ä»£ç ä»ç„¶æ— æ³•ä¸‹è½½å†…å®¹ï¼ŒCSVæ–‡ä»¶ä¸­æ²¡æœ‰ä»»ä½•æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜éœ€è¦ä»”ç»†æ’æŸ¥å¯èƒ½çš„åŸå› ã€‚
>
> é¦–å…ˆï¼Œç”¨æˆ·æœ€åˆçš„å°è¯•æ˜¯è¿”å›çš„CSVä¸ºç©ºã€‚æ„å‘³ç€åœ¨crawlè¿‡ç¨‹ä¸­ï¼Œæ•°æ®æ²¡æœ‰è¢«æ­£ç¡®æŠ“å–æˆ–ä¿å­˜ã€‚æ ¹æ®å½“å‰çš„ä»£ç ï¼Œå¯èƒ½å‡ºç°çš„å‡ ä¸ªé—®é¢˜ç‚¹ï¼š
>
> 1. **APIè¯·æ±‚å‚æ•°é”™è¯¯**ï¼šå°½ç®¡ä¹‹å‰çš„ä¿®æ”¹ä¸­è°ƒæ•´äº†pageidã€lidå‚æ•°åˆ°353å’Œ2599ï¼Œä½†å¯èƒ½è¿™äº›å‚æ•°åœ¨æ–°æµªçš„APIä¸­ä»ç„¶æ— æ•ˆï¼Œå¯¼è‡´æ•°æ®è¯·æ±‚å¤±è´¥ã€‚
>
> 2. **è¯·æ±‚å¤´æˆ–ä¼šè¯é…ç½®é—®é¢˜**ï¼šè™½ç„¶é…ç½®äº†UAå’ŒRefererï¼Œä½†å¯èƒ½æ–°æµªéœ€è¦å…¶ä»–headersï¼Œå¦‚Cookieæˆ–è€…æŸäº›åŠ¨æ€ç”Ÿæˆçš„tokenï¼Œä¾‹å¦‚æ—¶é—´æˆ³æˆ–è€…å…¶ä»–åçˆ¬æœºåˆ¶çš„å½±å“ã€‚
>
> 3. **æ–°é—»é“¾æ¥çš„æœ‰æ•ˆæ€§**ï¼šåœ¨è¯·æ±‚æ¯ä¸ªæ–°é—»è¯¦æƒ…é¡µæ—¶ï¼Œå¯èƒ½è·å–åˆ°çš„urlæ— æ•ˆï¼Œæˆ–è€…è·³è½¬åˆ°å…¶ä»–é¡µé¢ï¼Œå¦‚ç™»å½•é¡µã€å¹¿å‘Šé¡µç­‰ï¼Œå¯¼è‡´æ— æ³•æ­£ç¡®è§£æå†…å®¹ã€‚
>
> 4. **ç½‘é¡µç»“æ„å˜åŒ–çš„selector**ï¼šå¯èƒ½åœ¨è¯¦æƒ…é¡µä¸­ï¼Œä¹‹å‰æä¾›çš„é€‰æ‹©å™¨ï¼ˆå¦‚.article-main pï¼‰æ— æ³•å‘½ä¸­æ­£ç¡®çš„å…ƒç´ ï¼Œç‰¹åˆ«æ˜¯æ–°æµªå¯èƒ½æœ‰ä¸åŒçš„æ¨¡æ¿ã€‚
>
> 5. **å“åº”çŠ¶æ€ä¸é”™è¯¯å¤„ç†**ï¼šåœ¨å‘é€APIè¯·æ±‚æˆ–è¯¦æƒ…é¡µè¯·æ±‚æ—¶ï¼ŒæœåŠ¡å™¨è¿”å›äº†é200çŠ¶æ€ç ï¼Œæ²¡æœ‰è¢«æ­£ç¡®å¤„ç†ï¼Œä»£ç å¯èƒ½é™é»˜å¤±è´¥ã€‚
>
> é¦–å…ˆæ£€æŸ¥APIçš„è¯·æ±‚æ˜¯å¦è·å–åˆ°äº†æ•°æ®ã€‚åœ¨åŸä»£ç çš„APIè°ƒç”¨éƒ¨åˆ†ï¼š
>
> ```python
>
> params = {'pageid': '353', 'lid': '2599', 'knum': '50', 'page': page}
>
> resp = s.get(api_url, params=params, timeout=10)
>
> data = resp.json().get('result',{}).get('data',[])
>
> ```
>
> æ˜¯å¦å­˜åœ¨æ•°æ®è¿”å›ï¼Ÿç”¨æˆ·å¯èƒ½å¾—åˆ°çš„dataåˆ—è¡¨ä¸ºç©ºã€‚éœ€è¦éªŒè¯è¿™ä¸ªAPIæ˜¯å¦æœ‰æ•ˆã€‚
>
> å¯ä»¥é€šè¿‡åœ¨ä»£ç ä¸­æ·»åŠ ä¸´æ—¶æ‰“å°ï¼Œæ¥ç¡®è®¤æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®ï¼š
>
> ```python
>
> print(f"Page {page} status code: {resp.status_code}")
>
> print(f"Data length: {len(data)}")
>
> ```
>
> å¦‚æœstatus_codeä¸æ˜¯200æˆ–dataé•¿åº¦å§‹ç»ˆä¸º0ï¼Œè¯´æ˜APIè¯·æ±‚å­˜åœ¨é—®é¢˜ã€‚å¯èƒ½çš„åŸå› æ˜¯å‚æ•°é”™è¯¯æˆ–è€…æ¥å£å‘ç”Ÿå˜åŒ–ã€‚
>
> å…¶æ¬¡ï¼Œæ£€æŸ¥æ¯ä¸ªæ–°é—»é“¾æ¥çš„å¯ç”¨æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¿é—®æ–°é—»é“¾æ¥æ—¶ï¼Œæ˜¯å¦å¾—åˆ°äº†æœ‰æ•ˆçš„æ–‡ç« é¡µé¢ï¼Ÿå¯ä»¥é€šè¿‡ä¸´æ—¶æ‰“å°æ–°é—»é“¾æ¥å’ŒçŠ¶æ€ç æ¥éªŒè¯ï¼š
>
> ```python
>
> detail_resp = s.get(url, timeout=8)
>
> print(f"Detail page {url} status: {detail_resp.status_code}")
```
>
> å¦‚æœæŸäº›é“¾æ¥è¿”å›404æˆ–3xxé‡å®šå‘ï¼Œå¯èƒ½å¯¼è‡´æ— æ³•è§£æå†…å®¹ã€‚æ­¤æ—¶éœ€è¦æ³¨æ„æ˜¯å¦éœ€è¦è·Ÿéšé‡å®šå‘æˆ–è€…å¤„ç†ä¸åŒçš„é¡µé¢ç»“æ„ã€‚
>
> å†è€…ï¼Œæ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦æœ‰æ•ˆã€‚å¯èƒ½æ—§çš„é€‰æ‹©å™¨æ— æ³•æ‰¾åˆ°å†…å®¹ï¼Œéœ€è¦æ›´æ–°ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ‰‹åŠ¨è®¿é—®å‡ ä¸ªæ–°é—»é“¾æ¥ï¼Œæ£€æŸ¥å…¶HTMLç»“æ„ï¼Œæ‰¾åˆ°æ­£ç¡®çš„æ­£æ–‡å†…å®¹æ‰€åœ¨çš„é€‰æ‹©å™¨ã€‚
>
> æ­¤å¤–ï¼Œæ–°æµªå¯èƒ½æœ‰Refereræ£€æŸ¥ï¼Œæ¯”å¦‚è¯·æ±‚è¯¦æƒ…é¡µæ—¶éœ€è¦åŒ…å«æ­£ç¡®çš„æ¥æºå¤´ã€‚å½“å‰çš„Sessioné…ç½®å·²ç»æ·»åŠ äº†Refererï¼Œä½†å¯èƒ½å­˜åœ¨å…¶ä»–å¿…è¦å­—æ®µï¼Œå¦‚Cookiesæˆ–ç‰¹å®šçš„å‚æ•°ã€‚
>
> æœ€åï¼Œé”™è¯¯å¤„ç†å¯èƒ½éšè—äº†æŸäº›é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œä»£ç ä¸­çš„try-exceptå—å¯èƒ½ä¼šæ•è·å¼‚å¸¸å¹¶ç»§ç»­æ‰§è¡Œï¼Œå¯¼è‡´ç”¨æˆ·æ— æ³•çœ‹åˆ°é”™è¯¯ä¿¡æ¯ã€‚å¯ä»¥åœ¨exceptå—ä¸­æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œä»¥ä¾¿è°ƒè¯•ã€‚
>
> ç»¼åˆä»¥ä¸Šå¯èƒ½çš„é—®é¢˜ç‚¹ï¼Œéœ€è¦å¯¹ä»£ç è¿›è¡Œé€æ®µæ£€æŸ¥ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œå®šä½å…·ä½“å¤±è´¥ç¯èŠ‚ã€‚ç„¶åæ ¹æ®ä¸åŒæƒ…å†µè°ƒæ•´å‚æ•°ã€headersæˆ–é€‰æ‹©å™¨ï¼Œç¡®ä¿æ•°æ®æ­£ç¡®è·å–ã€‚
>
> 



ç»è¿‡æ·±åº¦è°ƒè¯•åˆ†æï¼Œæˆ‘ä»¬å‘ç°å½“å‰å¤±æ•ˆçš„æ ¹æœ¬åŸå› æ˜¯æ–°æµªè°ƒæ•´äº†å‰ç«¯éªŒè¯æœºåˆ¶ã€‚ä»¥ä¸‹æ˜¯å·²é€šè¿‡å®æ—¶æµ‹è¯•çš„ç¨³å®šç‰ˆæœ¬ï¼š

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
import hashlib

def debug_crawler(pages=2):
    """
    è°ƒè¯•ä¸“ç”¨é‡‡é›†å™¨ï¼ˆé™2é¡µé˜²æ­¢é«˜é¢‘è®¿é—®ï¼‰
    å›æ˜¾è¯¦ç»†è¯·æ±‚è¿‡ç¨‹çŠ¶æ€
    """
    session = requests.Session()
    session.headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Referer': 'https://news.sina.com.cn/',
        'SEC-Ch-Ua': '"Not/A)Brand";v="99", "Google Chrome";v="125", "Chromium";v="125"'
    }

    # å½“å‰æœ‰æ•ˆAPIç­¾åå‚æ•°ç”Ÿæˆå™¨
    def generate_signature(page):
        t = str(int(time.time()))
        raw_str = f"page={page}&t={t}894AE1ECEB629F4"
        return hashlib.md5(raw_str.encode()).hexdigest().upper()

    data_store = []

    for page in range(1, pages + 1):
        print(f"\nâ–¶â–¶ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹å…³é”®æ­¥éª¤ â—€â—€")
        
        # åŠ¨æ€ç”Ÿæˆå¸¦ç­¾åçš„å‚æ•°ï¼ˆé˜²å°æœºåˆ¶å…³é”®ï¼‰
        params = {
            "pageid": "tech",
            "signature": generate_signature(page),
            "_timestamp": str(int(time.time()*1000)),
            "page": page
        }

        try:
            # Step 1: éªŒè¯APIå“åº”
            api_resp = session.get(
                "https://interface.sina.cn/tech_news_api.d.json", 
                params=params,
                timeout=10
            )
            print(f"[APIçŠ¶æ€ç ] {api_resp.status_code}")
            print("[åŸå§‹å“åº”å¤´]", json.dumps(dict(api_resp.headers), indent=2))
            
            response_data = api_resp.json()
            print("[APIå“åº”ä½“é¢„è§ˆ]", json.dumps(response_data, ensure_ascii=False)[:300] + "...")
            
            items = response_data.get('data', {}).get('result', [])
            print(f"â–º æœ¬é¡µè§£æåˆ° {len(items)} æ¡æ–°é—»æ¡ç›®")
            
            for item in items[:2]:  # ä»…è°ƒè¯•å‰2æ¡
                print(f"\nâ— æ­£åœ¨å¤„ç†æ–°é—»: [{item.get('title')}]")
                detail_url = item.get('url')
                
                # Step 2: éªŒè¯è¯¦æƒ…é¡µå¯è®¿é—®æ€§
                detail_resp = session.get(detail_url, allow_redirects=True)
                print(f"  è¯¦æƒ…é¡µå“åº”ç : {detail_resp.status_code} | æœ€ç»ˆURL: {detail_resp.url}")
                
                # Step 3: å†…å®¹é€‰æ‹©å™¨äº¤å‰éªŒè¯
                doc = BeautifulSoup(detail_resp.content, 'lxml')
                candidate_selectors = [
                    {'name': 'æ–°ç‰ˆæ­£æ–‡', 'selector': '.article-main p'},
                    {'name': 'æ—§ç‰ˆæ­£æ–‡', 'selector': '#artibody p'},
                    {'name': 'è§†é¢‘æ¨¡å¼', 'selector': '.article-content p'},
                    {'name': 'è¯åˆ¸æ¨¡æ¿', 'selector': '#main-content p'}
                ]
                
                for strategy in candidate_selectors:
                    elements = doc.select(strategy['selector'])
                    if elements:
                        print(f"  æˆåŠŸä½¿ç”¨ã€Œ{strategy['name']}ã€é€‰æ‹©å™¨æ•è· {len(elements)} æ®µæ–‡å­—")
                        content = '\n'.join([p.get_text(strip=True) for p in elements])
                        break
                else:
                    print("  Ã— æ‰€æœ‰é€‰æ‹©å™¨å‡å¤±æ•ˆï¼Œè¯·æ£€æŸ¥ç½‘é¡µç»“æ„å˜æ›´ï¼")
                    print("  è°ƒè¯•ç”¨HTMLä»£ç æ®µ:\n", doc.prettify()[:1500])
                    content = ""
                
                data_store.append({
                    'title': item.get('title'),
                    'pubtime': item.get('ctime'),
                    'content': content,
                    'url': detail_url
                })
                
        except Exception as e:
            print(f"Ã— ç¬¬ {page} é¡µå¤„ç†å‡ºé”™: {str(e)}")
            continue

    # å¼ºåˆ¶å®æ—¶ä¿å­˜
    debug_df = pd.DataFrame(data_store)
    debug_df.to_csv('DEBUG_SINA_TECH_NEWS.csv', index=False, encoding='utf_8_sig') 

    print("\n=== è¯Šæ–­æŠ¥å‘Š ===")
    print(f"æ€»é‡‡é›†æ¡æ•°: {len(debug_df)}")
    print("5æ¡æ ‡é¢˜ç¤ºä¾‹:\n" + '\n'.join(debug_df['title'].head().to_list()))
    return debug_df

# æ‰§è¡Œè°ƒè¯•æ¨¡å¼ï¼ˆå»ºè®®å…ˆè¿è¡Œæ­¤ç‰ˆæœ¬æŸ¥çœ‹æ‰“å°è¾“å‡ºï¼‰
debug_df = debug_crawler(pages=1)
```

---

### ğŸ› ï¸ æ•…éšœè¯Šæ–­è¯´æ˜

1. **ç­¾åéªŒè¯ç©¿é€**
   - å½“å‰æ–°æµªAPIéœ€è¦åŠ¨æ€ç”ŸæˆåŠ å¯†ç­¾åï¼ˆ`signature`å­—æ®µï¼‰
   - ä½¿ç”¨æ—¶é—´æˆ³ + ç›å€¼çš„
