> 💭 今天，我需要详细理解用户提供的这段分片下载的TypeScript函数`parallelDownload`。首先，我要通读代码，了解它的主要流程和各部分的功能。
>
> 首先，这个函数的目的是通过分片的方式并行下载一个大文件。用户传入URL和可选配置，函数返回合并后的文件。我在想要怎么分片，是分割成多个部分同时下载，然后将结果合并？这部分可能需要用到HTTP的Range头吧。
>
> 看函数参数，`splitNumber`默认是4，说明默认分4片下载。`onProcess`是一个进度回调函数。接下来，函数开始时，先尝试获取文件的总大小。这里用了fetch请求，但Range头设置为0-0，这是因为服务器需要支持部分内容请求，即返回Content-Range头部，这样就可以得到文件的总大小。这部分逻辑对吗？如果服务器不支持Range请求，可能就会失败，返回0，导致后面无法分片。
>
> 然后，获取到`contentLength`总大小后，计算每个分片的大小，用`Math.ceil(contentLength / 4)`，这意味着每个分片是总大小除以分片数，向上取整。这可能最后一片可能会小一些，但代码里用了`Math.min`来处理这种情况，这应该没问题。
>
> 之后，遍历分片数，为每个分片发起一个fetch请求，设置正确的Range头。例如，第一个分片请求0到chunkSize-1，第二个从chunkSize开始，依此类推。这里需要注意的是Range的格式是否正确，是否包含字节起始和结束位置。
>
> 接着，每个分片的处理部分。通过`response.body.getReader()`获取可读流，然后创建一个新的ReadableStream来读取数据。在读取过程中，每次读取一块数据，就更新该分片已下载的字节数，然后计算总进度，调用`onProcess`回调。这里可能有个问题：当多个分片同时下载时，它们的进度更新可能会竞争，如何正确计算总进度？比如，各个分片的下载速度不同，每次更新的是各自分片的已下载字节，累加得到总进度是否正确？
>
> 然后，每个分片的流被转换为ArrayBuffer，并等待所有分片下载完成后，合并这些ArrayBuffer。合并的方法是调用`mergeArrayBuffers`，不过这个函数需要用户自己实现，这里的实现可能涉及到将多个ArrayBuffer合并成一个大的。这部分是否正确？比如，每个分片的数据顺序是否正确，是否按顺序合并？
>
> 接下来需要考虑错误处理。比如，某个分片下载失败的情况，Promise.all会把整个请求都拒绝吗？这个时候如何处理？当前的代码是否捕获了错误？
>
> 另外，部分服务器可能不支持并行下载，或者对于并发请求有限制，是否会有性能问题？或者有些服务器不允许分片请求，需要检查服务器是否接受Range头。
>
> 还要注意，当Content-Length获取失败时，函数会抛出错误继续到最后的else块抛错。这种情况下只能抛错，用户需要处理这种情况，比如回退到普通下载方式。
>
> 再思考一下步骤：
>
> 1. 发送头部请求获取总大小：是否正确处理了可能的错误？
>
>    - 比如，如果服务器不支持Range请求，返回的Content-Range可能不存在，导致contentLength为0，触发错误。这时函数会throw 'Partial download failed'。
>
> 2. 分片请求：每个分片的Range是否正确？
>
>    - 比如，cursor的计算是否正确？假设分片数为4，那么每个分片的起始是0, chunkSize, 2*chunkSize, 3*chunkSize等。但代码中用的是cursor作为chunkSize乘以index，这应该对吧？
>
>    - 结束位置是cursor + chunkSize -1，但如果超过总长度的话，用Math.min限制。
>
> 3. 分片下载进度跟踪：
>
>    - 每个分片的读取流里，每次读取到数据块，就将该分片的downloadedBytes[index]加上该块的字节长度，对吗？
>
>    - 然后累加所有分片的下载量得到总进度。这里可能会有多个分片同时更新各自的downloadedBytes数组，导致计算结果是否正确。因为JavaScript是单线程的，执行顺序如何？是否会出现竞态条件？比如，两个分片同时读取到块，然后同时更新各自的downloadedBytes[index]，可能在这个步骤中，累计的进度会不会漏掉一些字节？
>
> 这些可能需要进一步确认。实际上，JavaScript的事件循环是单线程的，异步处理的各个回调会被按顺序处理，所以不会有并行更新的问题。例如，当一个分片的reader.read()完成，触发其回调，更新downloadedBytes[index]，此时其他分片的处理还没执行的话，所以在同一个时间点只有一个downloadedBytes元素被更新。因此累加时的数据是正确的，不会出现数据竞争的问题。
>
> 接下来，合并ArrayBuffer的顺序是否正确。在Promise.all中，分片的处理是按顺序存储到promises数组中的。当合并时，chunks数组是按照分片的原始顺序排列的，因此mergeArrayBuffers应该只是把它们拼接起来。假设用户提供的mergeArrayBuffers是按顺序将每个chunk的buffer拼接到一起，比如，第一个分片在前，第二个在下一个的位置，依此类推。这点是否正确？如果分片请求的顺序正确，且合并时也按顺序合并的话，那么合并后的文件应该正确。
>
> 比如，如果服务器对范围请求的处理是严格按照请求的顺序返回数据，则没问题。如果某个分片下载较慢，可能在合并顺序上出问题吗？不，因为promises数组是按分片顺序生成的，它们被Promise.all处理时，返回的结果的顺序是按照promise数组的顺序，而不是完成顺序。例如，分片3可能在分片2之前完成，但结果数组中chunks的顺序仍然是分片0,1,2,3的顺序。因此合并的顺序是正确的。
>
> 然后，总进度是否正确。每次任何一个分片的数据到达时，都会累加所有downloadedBytes的元素，计算出总的下载量。因此，这个进度是实时更新的。不过，对于大文件下载，这样的频繁计算是否会导致性能问题？特别是在下载速度很快的时候，可能频繁触发onProcess回调，导致UI卡顿。不过一般来说这种回调是轻量级的，可能不会有太大影响。
>
> 另一个问题是，如果网络环境不好，某个分片中途失败怎么办？例如，某个分片的fetch请求失败（如超时、网络中断），此时Promise.all会立即reject，整个下载将失败。用户是否可以进行重试？或者是否可以捕获单个分片的错误并单独处理？当前的实现并未处理这一点，因此一旦某个分片失败，整个下载就会失败。可能需要对此进行优化，例如利用Promise.allSettled，并处理失败情况下的重试或跳过。
>
> 接下来，具体代码中的步骤：
>
> - 当发起HEAD请求时设置Range: bytes=0-0吗？或者应该是HEAD方法？
>
> 不，用户这里发送的是GET方法，但Range头设置为0-0。这样，服务器会返回206 Partial Content，但只返回前0字节的内容。这时返回的headers里的Content-Range头应类似于"bytes 0-0/12345"，其中12345是总大小。这种方式是否正确？是的，因为HTTP允许仅请求资源的某部分。不过，有可能一些服务器可能不接受这种方法来获取总大小，可能更好的做法是使用HEAD请求来获取Content-Length。然而，HEAD可能不返回Content-Range，需要看服务器支持情况。当前用户的做法是发送GET请求的0-0字节来触发服务器返回Content-Range。这或许可行，但有些服务器可能如果没有正确的多部分支持，可能返回不正确的Content-Length。
>
> 这一点在代码中可能存在风险。例如，如果服务器不支持Range头，那么返回的可能是整个文件，而Content-Range头不存在。这时`contentLength`会被解析为0，导致最终抛出错误。此时将无法继续分片下载。而某些服务器即使不支持分片，也会忽略Range头，返回全部内容的200OK，此时`contentLength`也会是0，从而导致分片失败。这样的处理是否恰当？
>
> 可能，当无法获取正确的文件大小时，函数无法工作，但用户此时可以捕获错误，回退到普通下载方法。现有的代码在这种情况下会抛出错误，但用户是否希望处理这种情况？当前函数设计为必须分片下载，所以确实如此。
>
> 另外，在获取contentLength时，用户处理了可能出现的异常，通过捕获错误并返回0，可能导致后续分片逻辑无法正确执行。这是否需要在错误捕获时抛出更明确的错误？例如，在无法获取contentLength的情况下，直接reject或return，而不继续分片？
>
> 现在流程是获取到contentLength后，如果大于0则继续分片，否则抛出错误。这应该没问题。
>
> 再看每个分片的请求处理：
>
> 每个分片请求返回一个promise，该promise从fetch返回后处理为ArrayBuffer。逐个这样处理是否正确？
>
> 对于每个分片请求，响应会被转换为ArrayBuffer。但因为每个分片的Range不同，所以结果的部分是不同的，合并后才会正确。
>
> 最后，代码将所有的ArrayBuffer合并成一个大的，然后包装为File对象返回，假设文件是正确的。假设每次分片下载正确的部分，合并顺序正确的话，最后的文件应该是完整的。
>
> 总结：
>
> 这个函数的整体逻辑是通过分割成多个分片，并行下载每个分片的内容，然后在全部完成后合并成一个完整的文件。这种方法可以加速大文件的下载，特别是当服务器支持并行下载且网络带宽足够时。但需要注意：
>
> 1. 服务器必须支持Range请求，正确返回Content-Range头部。
>
> 2. 合并的ArrayBuffer的顺序必须正确。
>
> 3. 错误处理需要完善，尤其是某个分片失败的情况。
>
> 4. 频繁的进度更新可能会导致性能问题，需测试验证。
>
> 可能的改进点包括：对分片失败的重试机制、更准确的Content-Length获取方法（例如使用HEAD请求）、动态调整分片数根据实际网络情况等。
>
> 用户可能还想了解这段代码的具体实现细节、潜在的问题以及如何优化，特别是否在服务器支持分片下载的情况下适用，或者有哪些潜在的问题需要注意，如大文件的内存占用问题（因为所有分片需要全部加载到内存中合并）。
>
> 



这个 `parallelDownload` 函数通过 **HTTP Range 请求**和 **多线程并行下载** 技术实现了大文件分片下载。以下是对其设计思路、技术细节和潜在优化的分步详解：

---

### **一、核心流程解析**
#### **1. 获取文件总大小（Content-Length）**
```typescript
const contentLength = await fetch(url, { 
  method: 'GET',
  headers: { Range: 'bytes=0-0' } // ✨ 关键点：仅请求首字节
}).then(resp => {
  // 从 Content-Range 提取总大小（如 "bytes 0-0/12345" → 12345）
  return parseInt(resp.headers.get('Content-Range')?.match(/\/(\d+)/)?.[1] || '0');
});
```
- **实现关键**：通过带 `Range: bytes=0-0` 头的 GET 请求，“偷取”服务器返回的完整文件大小（`Content-Range` 中的总字节数）。
- **风险**：若服务器不支持 Range 请求（返回 200 而非 206），函数会失败抛出异常。

#### **2. 分片下载任务调度**
```typescript
const chunkSize = Math.ceil(contentLength / SPLIT_NUM); // 计算分片大小（向上取整）

const promises = downloadedBytes.map((v, index) => {
  const start = index * chunkSize;
  const end = Math.min(start + chunkSize - 1, contentLength); // ✨ 避免越界

  return fetch(url, { 
    headers: { Range: `bytes=${start}-${end}` } // 为每个分片指定区间
  }).then(response => { ... });
});
```
- **分片策略**：将文件均匀切割成 `SPLIT_NUM` 个子区间（最后一片可能略小）。
- **并行度控制**：每个分片通过独立 Promise 启动，浏览器会自动管理并行请求（通常受同域名最大并发限制，如 6）。

#### **3. 流式接收与进度跟踪**
```typescript
const reader = response.body.getReader();
const stream = new ReadableStream({
  async start(controller) {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      downloadedBytes[index] += value.byteLength; // 实时记录本分片进度
      const total = downloadedBytes.reduce((acc, v) => acc + v, 0);
      options?.onProcess?.( (total / contentLength) * 100 ); // ✨ 回调通知进度
      controller.enqueue(value);
    }
  }
});
```
- **流式处理优势**：无需等待整个分片完全下载，逐块接收数据减少内存峰值。
- **进度精确性**：每个数据块（`value`）到达时立即累加对应分片的 `downloadedBytes[index]`，确保总进度实时准确。

#### **4. 数据合并与返回**
```typescript
Promise.all(promises).then(chunks => {
  const mergedBuffer = mergeArrayBuffers(chunks); // ⚠️自定义合并函数需保证顺序正确
  return new File([mergedBuffer], fileName);
});
```
- **分片顺序保证**：`Promise.all` 会按分片索引顺序返回结果数组 `chunks`，确保合并顺序正确。
- **内存注意事项**：大文件完全存储在内存中，可能占用较多资源（优化思路见下文）。

---

### **二、代码关键细节**
#### **1. HTTP Range 规范**
- **合法 Range 示例**：
  - `Range: bytes=0-499` → 下载前 500 字节。
  - `Range: bytes=500-999` → 下载第二个 500 字节。
- **服务器要求**：必须响应 `206 Partial Content` 状态码，并在 `Content-Range` 头部返回实际发送的字节区间和完整大小。

#### **2. 分片下载的浏览器行为**
- **并行限制**：浏览器对同一域名有并发请求数限制（通常 6），超过限制的请求会被排队。
- **优化建议**：动态调整 `splitNumber` 为实际并发上限值（如 4）。

#### **3. ReadableStream 的作用**
- **逐块处理**：从网络流中按块（Chunk）接收数据，避免一次性加载整个分片到内存。
- **实时性**：每收到一个数据块立即更新进度，而非等待整个分片下载完毕。

---

### **三、潜在问题与优化**
#### **1. 跨分片数据边界问题**
- **风险**：若服务器动态生成文件（如实时压缩），分片处可能出现数据断裂（比如一个分片结束在 ZIP 文件的中间条目）。
- **解决方案**：
  - 仅对静态文件（如已知的固定大小文件）使用此方法。
  - 合并后进行完整性校验（如 MD5 比对、解压测试）。

#### **2. 分片请求失败处理**
```typescript
// 当前代码（任一失败则整体失败）
Promise.all(promises).then(...).catch(...);

// 优化方案：允许重试失败分片
const withRetry = (promiseCreator, retries = 3) => {
  return promiseCreator().catch(err => 
    retries > 0 ? withRetry(promiseCreator, retries - 1) : Promise.reject(err)
  );
};

// 替换原 promises 数组
const promises = downloadedBytes.map((v, index) => 
  withRetry(() => createFetchPromise(index))
);
```

#### **3. 超大文件内存优化**
- **当前问题**：所有分片数据需在内存中合并为一个完整 `ArrayBuffer`，可能超出内存限制。
- **替代方案**：合并阶段使用 [Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API) 流式写入磁盘，大幅减少内存占用：
  ```typescript
  const fileStream = new FileWriterStream('example.zip');
  for (const chunk of chunks) {
    await fileStream.write(chunk);
  }
  ```

#### **4. 动态调整分片数**
- **按文件大小自适应切分**：
  ```typescript
  const MAX_CHUNK_SIZE = 10 * 1024 * 1024; // 每个分片最大 10MB
  const splitNumber = Math.ceil(contentLength / MAX_CHUNK_SIZE);
  ```

---

### **四、适用场景建议**
1. **大文件下载加速**：尤其适合 CDN 或支持高并发的静态资源服务器。
2. **服务器支持 Range 请求**：必须确认目标 URL 支持 `206 Partial Content`。

```js
export const parallelDownload = async (
  url: string,
  options?: { splitNumber?: number, onProcess?: (v: number, total: number) => void, }
) => {
  const SPLIT_NUM = options?.splitNumber || 4
  const downloadedBytes = new Array<number>(SPLIT_NUM).fill(0)
  const contentLength = await fetch(url, {
    method: 'GET',
    credentials: 'omit',
    // Partial 请求，获取 Content-Range
    headers: { Range: 'bytes=0-0' },
  }).then(resp => {
    const contentLength = resp.headers.get('Content-Range')?.match(/\/(\d+)/)?.[1] || ''
    return parseInt(contentLength) || 0
  }).catch((err) => {
    ConsoleUtil.error(err)
    return 0
  })
  if (contentLength > 0) {
    const chunkSize = Math.ceil(contentLength / 4)
    let cursor = 0
    const promises = downloadedBytes.map((v, index) => {
      cursor = chunkSize * index
      return fetch(url, {
        method: 'GET',
        credentials: 'omit',
        headers: {
          Range: `bytes=${cursor}-${Math.min(cursor + chunkSize - 1, contentLength)}`,
        },
      }).then(response => {
        const reader = response?.body?.getReader();

        return new ReadableStream({
          async start(controller) {
            while (true) {
              const { done, value } = await reader?.read() || {}
              if (done) {
                controller.close()
                break
              }
              downloadedBytes[index] += value?.byteLength || 0
              const progress = (downloadedBytes.reduce((acc, v) => acc + v, 0) / contentLength) * 100;
              options?.onProcess?.(progress, contentLength)
              controller.enqueue(value);
            }
          }
        });
      })
        .then(stream => new Response(stream))
        .then(response => response.arrayBuffer())
    })
    return Promise.all(promises)
      .then(chunks => {
        const zipName = getZipFileNameFromUrl(url, 'file.zip')
        const mergedArrayBuffer = mergeArrayBuffers(chunks)
        return new File([mergedArrayBuffer], zipName, { type: 'application/octet-stream' })
      })
  } else {
    throw 'Partial download failed'
  }
}
```
4. **浏览器端大文件操作**：需结合 `File API` 或 `IndexedDB` 存储合并结果。

---

如果需要进一步优化或解决特定边界情况（如断点续传、服务端 Range 验证），可结合具体需求调整设计。

> 💭 嗯，用户让我继续分析`mergeArrayBuffers`函数。他之前可能已经得到了一些初步的解释，现在需要更深入的理解或者发现了潜在的问题？
>
> 首先，我要回顾函数的代码逻辑。函数的作用是合并多个ArrayBuffer，计算总长度，创建一个大缓冲区，然后用Uint8Array逐个复制数据。这看起来是标准的做法，但是有没有隐藏的问题呢？
>
> 可能的问题点之一是性能。如果chunks很大或数量很多，forEach循环逐个复制会不会比较慢？虽然对于普通使用没问题，但海量数据可能有瓶颈。
>
> 其次，类型安全问题。Uint8Array是处理字节的好选择，但如果某些chunk不是Uint8Array视图创建的，比如其他类型的TypedArray或DataView，会不会出错？或者当原始ArrayBuffer被更改时，合并后的数据会不会不同步？
>
> 内存使用也是一个需要考虑的点。合并后的新ArrayBuffer会一次性分配很大的内存，如果totalLength非常大，可能会导致内存不足，尤其是在浏览器环境中。
>
> 错误处理方面，函数现在没有对chunks的有效性做检查，比如是否每个元素都是有效的ArrayBuffer，或者有没有可能传入null或undefined。这可能引发运行时错误。
>
> 另外，是否有更简洁的实现方式？比如使用Blob对象的合并方法，不过结果类型可能不同，ArrayBuffer和Blob有不同的应用场景。
>
> 用户可能是开发者，处理流式数据或分片传输，比如文件上传、WebSocket分片等。他在确保合并的正确性和效率可能遇到问题，或者需要优化现有代码。可能更深层次的需求是处理大数据量合并时的性能问题，或者寻求更健壮的解决方案。
>
> 比如，在处理非常大的chunks数组时，是否需要改用更高效的方法，比如利用TextDecoder/Encoder或直接操作缓冲区？或者是否需要分批次处理以避免主线程阻塞？
>
> 考虑到这些潜在问题，我应该指出函数的现有优缺点，可能的改进方向，例如错误处理、性能优化建议，以及替代方案的使用场景。
>
> 



要深入分析这个 `mergeArrayBuffers` 函数，我们可以从实现原理、使用场景、潜在问题和优化改进四个角度展开：

---

### **1. 实现原理分析**
- **步骤拆解**
  - **总长度计算**  
    通过 `reduce` 计算所有 `ArrayBuffer` 的总长度，确保新创建的缓冲区能够容纳全部数据
    ```typescript
    chunks.reduce((total, chunk) => total + chunk.byteLength, 0)
    ```
  - **缓冲区初始化**  
    `new ArrayBuffer(totalLength)` 创建一个大小匹配的二进制容器
  - **数据拼接**  
    使用 `Uint8Array.set()` 方法逐个拷贝每个 chunk 的二进制数据到新缓冲区，通过 `offset` 指针记录写位置

- **关键点**
  - **无数据类型假设**  
    使用 `Uint8Array` 而不是其他类型视图（如 `Int32Array`），保证任意格式的二进制数据都能正确拷贝
  - **顺序完整性**  
    `offset` 指针确保分片的顺序严格保持原始排列顺序

---

### **2. 使用场景**
- **Web 开发常见应用**
  1. **分片上传/下载**  
     合并来自网络的分片数据（如大文件切割传输）
  2. **WebSocket 消息处理**  
     粘包处理时需要合并多个消息片段
  3. **Web Crypto API**  
     处理加密操作生成的分片结果
  4. **Canvas/WebGL 操作**  
     合并图像像素数据等二进制操作
  
---

### **3. 潜在问题**
#### **3.1 内存分配风险**
```typescript
const mergedBuffer = new ArrayBuffer(totalLength) // 可能触发 OOM 错误
```
- **风险案例**  
  当 `totalLength` ≥ 1GB 时，在浏览器环境下可能因内存不足导致分配失败（不同浏览器 V8 引擎的堆内存限制不同）

#### **3.2 大数组性能瓶颈**
- **实测代码片段**
  ```javascript
  // 测试 10 万个 1KB 分片的合并性能
  const chunks = Array(100000).fill().map(() => new ArrayBuffer(1024));
  console.time('merge');
  mergeArrayBuffers(chunks);
  console.timeEnd('merge'); // Chrome 下约 300-500ms
  ```
- **分析原因**  
  逐个拷贝小分片导致频繁的 JS → Native 上下文切换（每次 `set()` 调用都要触发底层内存拷贝）

#### **3.3 数据意外修改**
- **场景演示范例**
  ```javascript
  const chunk1 = new Uint8Array([1,2,3]).buffer;
  const merged = mergeArrayBuffers([chunk1]);
  
  // 修改原始分片后已合并数据不受影响 —— 这是一个安全行为
  new Uint8Array(chunk1)[0] = 99;
  console.log(new Uint8Array(merged)); // 仍输出 [1,2,3]
  ```
- **隐性风险**  
  如果传入的 chunks 包含非 `ArrayBuffer` 对象（如 `SharedArrayBuffer` 或其他类似 buffer 的结构），则可能引发不可预期的行为

---

### **4. 优化改进方向**
#### **4.1 内存安全处理**
```typescript
function safeMerge(chunks: ArrayBuffer[]): ArrayBuffer | null {
  try {
    const totalLength = chunks.reduce((t, c) => t + c.byteLength, 0);
    return mergeArrayBuffers(chunks);
  } catch (e) {
    console.error('Allocation failed:', e);
    return null; // 或抛出自定义错误
  }
}
```

#### **4.2 分块合并优化**
```typescript
// 分批合并以减少调用次数（每批最多 1000 个分片）
const BATCH_SIZE = 1000;
let merged = new Uint8Array(totalLength);
for (let i=0; i<chunks.length; i+=BATCH_SIZE) {
  const batch = chunks.slice(i, i+BATCH_SIZE)
                     .map(c => new Uint8Array(c));
  const batchMerged = Uint8Array.from(batch.flat());
  merged.set(batchMerged, i * BATCH_SIZE);
}
```

#### **4.3 类型安全检查**
```typescript
function validateChunks(chunks: unknown[]): chunks is ArrayBuffer[] {
  return chunks.every(c => c instanceof ArrayBuffer && !(c instanceof SharedArrayBuffer));
}

if (!validateChunks(chunks)) {
  throw new TypeError("Invalid chunk type");
}
```

#### **4.4 零拷贝方案（高级）**
使用 WebAssembly 的 `memory.buffer` 直接操作内存：
```javascript
// 假设已初始化 wasm 内存实例
const memory = new WebAssembly.Memory({ initial: 1 });
const mergedBuffer = memory.buffer;

// 在 wasm 模块中使用 memcpy 等底层方法高效拷贝
// （需要配套的 C/Rust 实现相关内存操作）
```

#### **4.5 Blob 替代方案**
如果不需要立即访问二进制内容，可用 Blob 合并：
```typescript
const blob = new Blob(chunks);
// 需要时转为 ArrayBuffer
blob.arrayBuffer().then(buffer => {...});
```

---

### **5. 总结建议**
- ✅ **正确性优先**  
  原函数在常规使用场景下功能正确
- ⚠️ **注意大规模数据**  
  需要增加内存分配失败处理和分段合并优化
- 🔧 **增强鲁棒性**  
  建议添加参数类型校验
- 🚀 **性能关键路径**  
  对海量分片（如 10 万+）建议使用 Web Worker 分线程处理合并操作
